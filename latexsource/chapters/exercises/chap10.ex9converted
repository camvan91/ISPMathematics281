

\smallskip


\begin{enumerate}

\item  Find a subset of the following set of vectors which
is a basis for the subspace it spans.
\[
\left\{
\bm{} 1\\ 2 \\ 3 \\ 0 \em,
\bm{} 3\\ 0 \\ -3 \\ 2 \em,
\bm{} 3\\ 3 \\ 3 \\ 1 \em,
\bm{} 1\\ -1 \\ -3 \\ 1 \em
\right\}
\]

\answer{  Gauss--Jordan reduction of the matix with these columns
yields
\[
\bm 1 & 0 & 3/2 & -1/2 \\ 0 & 1 & 1/2 & 1/2 \\ 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 \em
\]
so the first two vectors in the set form a basis for the subspace
spanned by the set.


}

\item   Let $\displaystyle{A = \bm{}
1 & 0 & 2 & 1 & 1 \\
-1 & 1 & 3 & 0 & 1 \\
1 & 1 & 7 & 2 & 3 \em
}$.

(a)  Find a basis  for the column space of $A$.

(b)  Find a basis for the row space of $A$.

\answer{ (a)  Gauss-Jordan reduction yields
\[
\bm 1 & 0 & 2 & 1 & 1 \\ 0 & 1 & 5 & 1 & 2 \\ 0& 0 & 0 & 0 & 0 \em
\]
so
\[
\left\{\bm 1\\ -1 \\ 1 \em,\bm 0 \\ 1 \\ 1 \em\right\}
\]
is a basis.\par
(b) A basis for the row space is
\[
\{ \bm 1 & 0 & 2 & 1 & 1 \em, \bm 0 & 1 & 5 & 1 &2 \em\}.
\]\par
Note that neither of these has any obvious connection to the
solution space which has basis
\[
\left\{ 
\bm 2\\ 5 \\ 1 \\ 0 \\ 0 \em,
\bm 1\\ 1 \\ 0 \\ 1 \\ 0 \em,
\bm 1\\ 2 \\ 0 \\ 0 \\ 1 \em \right\}.
\]


}

\item    Let
\[
\v_1 = \bm{} 1\\ -2 \\ -1 \em,\qquad \v_2 = \bm 1 \\ 2 \\  1 \em.
\]
Find a basis for $\R^3$ by finding a third vector $\v_3$ such
that $\{\v_1, \v_2, \v_3\}$ is linearly independent.   Hint.  You may
find an easier way to do it, but the following method should work.
Use the method suggested in Section 9 to pick out a linearly independent
subset from $\{\v_1, \v_2, \e_1, \e_2, \e_3\}$.

\answer{   Reduce
\[
\bm 1 & 1 & 1 & 0 & 0 \\ -2 & 2 & 0 & 1 & 0 \\ -1 & 1 & 0 & 0 & 1 \em
\]
to get
\[
\bm 1 & 0 & 1/2 & 0 & -1/2 \\ 0 & 1 & 1/2 & 0 & 1/2 \\ 0 & 0 & 0 & 1 & -2 \em.
\]
Picking out the first, second, and fourth
columns shows that $\{\v_1,\v_2, \e_2\}$ is a basis for $\R^3$
containing $\v_1$ and $\v_2$.


}

\item    Let $\{\v_1, \v_2, \dots, \v_k\}$ be a linearly independent
subset of $\R^n$.   Apply the method in section 9 to the set
$\{\v_1, \v_2, \dots, \v_k, \e_1, \e_2, \dots, \e_n\}$.   It
will necessarily yield a basis for $\R^n$.  Why?   Show that this
basis will include $\{\v_1, \v_2, \dots, \v_k\}$ as a subset.
That is show that none of the $\v_i$ will be eliminated by the
process.

Note.  If $\V$ is any finite dimensional vector space with
basis $\{\u_1,\u_2, \dots, \u_n\}$ and $\{\v_1, \v_2, \dots,\v_k\}$
is a linearly independent subset of $\V$, then we may form another
basis for $\V$ by adding {\it some\/} of the vectors
$\u_1, \u_2, \dots, \u_n$ to $\{\v_1, \v_2, \dots,\v_k\}$.
However, since we aren't necessarily dealing with column vectors
in this case, the proof is a bit more involved.

\item    Show that
\[
\u_1 = \bm{} 1\\ -1 \em\qquad\text{and}\qquad \u_2 = \bm 1\\ 1 \em
\]
form a linearly independent pair in $\R^2$.   It follows that
they form a basis for $\R^2$.  Why?   Find the coordinates of
$\e_1$ and $\e_2$ with respect to this new basis.   Hint.
You need to solve
\[
\bm \u_1 &  \u_2 \em \bm x_1\\ x_2 \em = \e_1
\qquad\text{and}\qquad
\bm \u_1 &  \u_2 \em \bm x_1\\ x_2 \em = \e_2.
\]
You can solve these simultaneously by solving
\[
\bm \u_1 & \u_2 \em X = I
\]
for an appropriate $2\times 2$ matrix $X$.   What does this have
to do with inverses?

\answer{ It is clear that the vectors form a linearly independent
pair since neither is a multiple of the other.  To find the
coordinates of $\e_1$ with respect to this new basis, solve
\[
\bm 1 & 1 \\ -1 & 1 \em \bm x_1\\ x_2\em = \bm 1 \\ 0 \em.
\]
The solution is $x_1 = x_2 = 1/2$.  Hence, the coordinates
are given by
\[
\bm 1/2\\ 1/2 \em.
\]
Similarly, solving
\[
\bm 1 & 1 \\ -1 & 1 \em \bm x_1\\ x_2\em = \bm 0 \\ 1 \em
\]
yields the following coordinates for $\e_2$.
\[
\bm -1/2\\ 1/2 \em.
\]
One could have found both sets of coordinates simultaneously
by solving
\[
\bm 1 & 1 \\ -1 & 1 \em X = \bm 1& 0 \\ 0 & 1 \em
\]
which amounts to finding the inverse of the matrix $\bm \u_1 & \u_2 \em$.


}



\end{enumerate}
\endinput
