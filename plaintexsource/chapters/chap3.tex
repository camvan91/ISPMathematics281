\documentstyle{book}
\input epsf.tex
\input extra.tex
\input le2.sty
\input chap2.lnk
\maketoctrue
\indextrue
\mardiagtrue
\openseg{chap3}
\Monograph
\topmatter
\nextchap{Differential Calculus of Functions of $n$ Variables}
\title\chapter{\chapno} Differential Calculus of Functions of
$n$ Variables\endtitle
\endtopmatter
\document

We want to develop the calculus necessary to discuss functions
of many variables.   We shall start with functions
$f(x,y)$ of two independent
variables and functions $f(x,y,z)$ of three independent
variables.   However, in general, we need to consider functions
$f(x_1, x_2, \dots, x_n)$ of any number 
of independent variables.   We shall use the notation $\R^n$ as
before to stand for the set of all $n$-tuples $(x_1,x_2,\dots,x_n)$
with real entries $x_i$.  For $n = 2$, we shall identify 
$\R^2$ with the plane and, for $n = 3$, we shall identify $\R^3$
with space.   We shall use the old fashioned term {\it locus\/}
to denote the set of all points satisfying some equation or condition.
\outind{locus}%
  

\nextsec{Graphing in $\noexpand\R^n$}
\head Section \sn.   Graphing in $\R^3$ \endhead  

We shall encounter equations involving two, three, or more variables.
As you know, an equation of the form
$$
     f(x,y) = C
$$
may be viewed as defining a curve in the plane.  For example,
$ax + by = c$ has plane locus a line, while $x^2 + y^2 = R^2$
has plane locus a circle of radius $R$ centered at the origin.
Similarly, an equation involving three variables
$$
   f(x,y,z) = C
$$
may be thought of as defining a {\it surface\/} in space.
Thus, we saw previously that the locus in $\R^3$ of a linear equation
$$
   ax + by + cz = d
$$
(where not all $a, b$, and $c$ are zero) is a plane.   If we
use more complicated equations, we get more complicated surfaces.

\nextex
\example{Example \en}
The equation
$$
   x^2 + y^2 + z^2 = R^2
$$
may be rewritten $|\r| = \sqrt{x^2 + y^2 + z^2} = R$, so it
asserts that the point with position vector $\r$ is at distance
$R$ from the origin.  Hence, the locus of all such points is a
{\it sphere\/} of radius $R$ centered at the origin.
\endexample
\medskip
\centerline{\epsfbox{s3-1.ps}}
\medskip
\nextex
\example{Example \en}
Consider the locus of the equation
$$
  x^2 + 2x + y^2 - 4y + z^2 = 20.
$$
This is also a sphere, but one not centered at the origin.  To
see this, {\it complete the squares\/} for the terms involving
$x$ and  $y$.
$$
\align
    x^2 + 2x + 1 + y^2 -4y + 4 + z^2 &= 10 + 1 + 4 = 25 \\
    (x + 1)^2 + (y - 2)^2 + z^2 &= 5^2.
\endalign$$
This asserts that the point with position vector $\r = \lb x, y, z \rb$
is 5 units from the point $(-1, 2, 0)$, i.e., it lies on a sphere
of radius 5 centered at $(-1, 2, 0)$.
\endexample

\nextex
\example{Example \en}
Consider the locus of the equation $z = x^2 + y^2$ (which could
also be written $x^2 + y^2 - z = 0$.)  To see what this looks like,
we consider its intersection with various planes.   Its intersection
with the $y,z$-plane is obtained by setting $x = 0$ to get
$z = y^2$.  This is a parabola in the $y,z$-plane.  
Similarly, its intersection with the $x,z$-plane is the parabola
given by $z = x^2$.   To fill in the picture,  consider
intersections with planes parallel to the $x,y$-plane.  Any
such plane has equation $z = h$, so the intersection has equation
$x^2 + y^2 = h = (\sqrt h)^2$, which you should recognize as a circle
of radius $\sqrt h$, at least if $h > 0$.  Note that the circle is
centered at $(0,0,h)$ on the $z$-axis since it lies in the plane $z = h$.
If $z = h = 0$, the circle reduces to a single point, and for 
$z = h < 0$, there is no locus.
The surface is ``bowl'' shaped.  It is called a {\it circular
paraboloid}.
\endexample

\mar{s3-2.ps}
Graphing a surface in $\R^3$ by sketching its traces on various
planes is a useful strategy.  In order to be good at it, you need
to know the basics of plane analytic geometry so you can recognize
the resulting curves.  In particular, you should be familiar with
the elementary facts concerning {\it conic sections\/}, i.e.,
ellipses, hyperbolas, and parabolas.   Edwards and Penney, 3rd Edition,
Chapter 10 is a good reference for this material.

\nextex
\example{Example \en}
Consider the locus in space of $\displaystyle{\frac{x^2}4 +
\frac{y^2} 9} = 1$.   Its intersection with a plane $z = h$
parallel to the $x,y$-plane is an ellipse centered on the $z$-axis
and with semi-minor and semi-major axes  2 and 3.   The surface
is a {\it cylinder\/} perpendicular to the $x,y$-plane with
elliptical cross sections.
Note that the locus {\it in space\/} is not just the ellipse
in the $x,y$-plane with the same equation.
\endexample
\medskip
\centerline{\epsfbox{s3-3.ps}}
\medskip
\nextex
\example{Example \en}
Consider the locus in space of the equation
$z = \dfrac 1{x^2 + y^2}$.   Its intersection with the plane $z = h$
(for $h > 0$) is the circle with equation
$ x^2 + y^2 = 1/h = (\sqrt{1/h})^2$.   The surface does not intersect
the $x,y$-plane itself ($z = 0$) nor any plane below the $x,y$-plane.
It intersection with the $x,z$-plane ($y = 0$) is the curve
$z = 1/x^2$ which is asymptotic to the $x$-axis
and to the positive $z$-axis.  Similarly, for its intersection
with the $y,z$-plane   The surface flattens out and approaches the
$x,y$-plane as $r = \sqrt{x^2 + y^2} \to \infty$.  It approaches the
positive $z$-axis as $r \to 0$.
%D
\endexample

\nextex
\example{Example \en}
Consider the locus in space of the equation $yz = 1$.   Its intersection
with a plane parallel to the $y,z$-plane ($x = d$) is a hyperbola
asymptotic to the $y$ and $z$ axes.  The surface is perpendicular
to the $y,z$-plane.  Such a surface is also called a {\it cylinder\/}
although it doesn't close upon itself as the elliptical cylinder considered
above.
\endexample

\nextex
\example{Example \en}
Consider the locus of the equation $x^2 + z^2 = y^2 - 1$.  For
each plane parallel to the $x,z$-plane ($y = c$), the intersection
is a circle $x^2 + z^2 = c^2 - 1 = (\sqrt{c^2 - 1})^2$ centered
on the $y$-axis, at least of $c^2 > 1$.  For $y = c = \pm 1$,
the locus is a point, and for $-1 < y = c < 1$, the locus is
empty.   In addition, the intersection of the surface with the
$x,y$-plane ($z = 0$) is the hyperbola with equation $x^2 - y^2
 = -1$, and similarly for its intersection with the $y,z$-plane.
The surface comes in two pieces which open up as ``bowls'' centered
on the positive and negative $y$-axes.   The surface is called
a {\it hyperboloid of 2 sheets}.    
\medskip
\centerline{\epsfbox{s3-4.ps}}
\medskip
\endexample
\subhead Graphs of Functions \endsubhead
For a scalar function $f$ of one independent variable, 
the {\it graph of the function\/} is the set of
\outind{graph of a function}%
all points in $\R^2$  of the form  $(x, f(x))$
for $x$ in the domain of the function.   (The domain of a function
is the set of values of the independent variable for which the
\outind{domain of a function}%
function is defined.)  In other words, it is the locus of the
equation $y = f(x)$.  It is generally a curve in the plane.

We can define a similar notion for a scalar function $f$ of two
independent variables.  The graph is the
set of points in $\R^3$  of the form $(x, y, f(x,y))$
for $(x,y)$ a point in the domain of the function.  In other words,
it is the locus of the equation $z = f(x,y)$, and it is generally
a surface in space.
The graph of a function is often useful in understanding the
function.

\mar{s3-5.ps}
We have already encountered several examples of graphs of functions.
For example, the locus of $z = x^2 + y^2$ is the graph of the
function $f$ defined by $f(x,y) = x^2 +y^2$.   Similarly, the locus
of $z = 1/(x^2 + y^2)$ is the graph of the function $f$ defined by
$f(x,y) = 1/(x^2 + y^2)$ for $(x,y) \not= (0,0)$. 
  Note that in the first case there need
be no restriction on the domain of the function, but in the second
case $(0,0)$ was omitted.  

In some of the other examples, the locus
of the equation cannot be considered the graph of a function.
For example, the equation $x^2 + y^2 + z^2 = R^2$ cannot be solved
uniquely for $z$ in terms of $(x,y)$.  Indeed, we have
$z = \pm\sqrt{R^2 - x^2 - y^2}$, so that two possible functions
suggest themselves.  $z = f_1(x,y) = \sqrt{R^2 - x^2 - y^2}$ 
defines a function with graph
the {\it top hemisphere\/} of the sphere, while $z = f_2(x,y)
= - \sqrt{R^2 - x^2 - y^2}$ yields the lower hemisphere.  (Note that
for either of the functions the relevant domain is the set of
points on or inside the circle $x^2 + y^2 = R^2$.  For points outside
that circle, the expression inside the square root is negative, so,
since we are only talking about functions assuming real values,
such points must be excluded.)

\mar{s3-6.ps}
\smallskip
\nextex
\example{Example \en}
Let $f(x,y) = xy$ for all $(x,y)$ in $\R^2$.    The graph is the
locus of the the equation $z = xy$.  We can sketch it by considering
traces on various planes.  Its intersection with a plane parallel
to the $x,y$-plane ($z =$ constant) is a hyperbola asymptotic to
lines parallel to the $x$ and $y$ axes.   For $z > 0$, the hyperbola
is in the first and third quadrants of the plane, but for
$z < 0$ it is in the second and fourth quadrants.  For $z = 0$,
the equation is $xy = 0$ with locus consisting of the $x$-axis
($y = 0$) and the $y$-axis ($x = 0$).   Thus, the graph intersects
the $x,y$-plane in two straight lines.  Th surface is generally
shaped like an ``infinite saddle''.  It is called a {\it hyperbolic
paraboloid}.  It is clear where the term ``hyperbolic'' comes from.
Can you see any parabolas?  (Hint: Try planes perpendicular to
the $x,y$-plane with equations of the form  $y = mx$.)
\endexample
\centerline{\epsfbox{s3-7.ps}}
\medskip
\nextex
\example{Example \en}
 Let $f(x,y) = x/y$ for $y \not= 0$.  Thus, the domain of this
function consists of all points $(x,y)$ not on the $x$-axis
($y = 0$).
The trace in the plane $y = c, c \not= 0$ is the line $z = (1/c)x$
with slope $1/c$.   Similarly, the trace in the plane $z = c, c \not=0$
is the line $y = (1/c)x$.  Finally, the trace in the plane $x = c$,
is the hyperbola $z = c/x$.  Even with this information you will have
some trouble visualizing the graph.   However, the equation
$z = x/y$ can be rewritten $yz = x$.   By permuting the variables,
you should see that the locus of $yz = x$
 is similar to the  saddle shaped surface
we just described, but oriented differently in space.  However,
 the saddle
is not quite the graph of the function since it  contains the 
 $z$-axis ($y = x = 0$) but the graph of the function does not.
In general, the graph of a function, since it consists of points
of the form $(x,y,f(x,y))$, cannot contain points with the same
values for $x$ and $y$ but different values for $z$.  In other words,
any line parallel to the $z$-axis can intersect such a graph at most
once. 
%D  z = x/y
\endexample

Sketching graphs of functions, or more generally loci of equations
in $x, y$, and $z$, is not easy.   One approach drawn from the
study of topography is to interpret the equation $z = f(x,y)$ as
giving the {\it elevation\/} of the surface, viewed as a hilly
terrain, above a reference plane.  (Negative elevation
 $f(x,y)$ is
interpreted to mean that the surface dips below the reference plane.)
For each possible elevation $c$,
 the intersection of the plane $z = c$ with the graph
yields a curve $f(x,y) = c$.  This curve is called a {\it level
curve}, and we draw a 2-dimensional map of the graph by sketching
\outind{level curve}%
the level curves and labeling each by the appropriate elevation
$c$.  Of course, there are generally infinitely many level curves
since there are infinitely many possible values of $z$, but we
select some subset to help us understand the topography of the
surface.
\medskip
\centerline{\epsfbox{s3-8.ps}}
\medskip
\nextex
\example{Example \en}
The level curves of the surface $z = xy$ have equations
$xy = c$ for various $c$.  They form a 
a family of hyperbolas, each with two branches.  For $c > 0$,
these hyperbolas fill the first and third quadrants, and for
$c < 0$ they fill the second and fourth quadrants.  For $c  = 0$
the $x$ and $y$ axes together constitute the level ``curve''.
See the diagram.
%D  Level curves of saddle.
You can see that the region around the origin $(0,0)$ is like a
``mountain pass'' with the topography rising in the first and
third quadrants and dropping off in the second and fourth quadrants.
In general a point where the graph behaves this way is called
a {\it saddle point}.   Saddle points indicate the added complexity
which can arise when one goes from functions of one variable to
functions of two or more variables.  At such points, the function
can be considered as having a maximum  or
 a minimum depending on where you
look.
\endexample
\smallskip
\mar{s3-9.ps}
\smallskip

\subhead Quadric Surfaces \endsubhead
One important class of surfaces are those defined by quad\-rat\-ic
equations.  These are analogues in three dimensions of
conics in two dimensions.  They are called {\it quadric
surfaces}.  We describe here {\it some\/} of the possibilities.
You can verify the pictures by using the methods described above.
\outind{quadric surface}%

Consider first equations of the form
$$
   \pm \frac{x^2}{a^2} 
   \pm \frac{y^2}{b^2} 
   \pm \frac{z^2}{c^2} = 1
$$

If all the signs are positive, the surface is called an {\it 
ellipsoid}.
\outind{ellipsoid}%
Planes perpendicular to one of the coordinate axes intersect it
in ellipses (if they intersect at all).   However, at the extremes
these ellipses degenerate into the points $(\pm a, 0, 0),
(0,\pm b, 0)$, and $(0,0,\pm c)$.
\medskip
\centerline{\epsfbox{s3-10.ps}}
\medskip
If exactly one of the signs are negative, the surface is called a
{\it hyperboloid of one sheet}.   It is centered on one axis
\outind{hyperboloid of one sheet}%
(the one associated to the negative coefficient in the equation)
and it opens up in both positive and negative directions along
that axis.  Its intersection with planes perpendicular to that
axis are ellipses.   Its intersections with planes perpendicular to
the other axes are hyperbolas.
%D hyperboloid of one sheet

If exactly two of the signs are negative, the surface is called
a {\it hyperboloid of two sheets}.   It is centered on one
\outind{hyperboloid of two sheets}%
axis (associated to the positive coefficient).  For example,
suppose the equation is
$$
  -\frac{x^2}{a^2}
  +\frac{y^2}{b^2}
  -\frac{z^2}{c^2} = 1.
$$
For $y < -b$ or $y > b$, the graph intersects a plane perpendicular
to the $y$-axis in an ellipse.  For $y = \pm b$, the intersection
is the point $(0,\pm b, 0)$.  (These two points are called vertices
of the surface.)  For $-b < y < b$, there is no intersection with
a plane perpendicular to the $y$-axis.
%D hyperboloid of two sheets.

The above surfaces are called {\it central quadrics\/}.
\outind{central quadric}%
Note that for the hyperboloids, with equations in standard form as
above, the number of sheets is the same as
the number of minus signs.

Consider next equations of the form
$$
   z = \pm \frac{x^2}{a^2} \pm \frac{y^2}{b^2}
$$
(or similar equations obtained by permuting $x, y$ and $z$.)

If both signs are the same, the surface is called an {\it elliptic
paraboloid}.   If both signs are positive, it is centered on the
\outind{elliptic paraboloid}%
\outind{paraboloid, elliptic}%
positive $z$-axis and its intersections with planes perpendicular to
the positive $z$-axis are a family of similar ellipses which increase
in size as $z$ increases.   If both signs are negative, the situation
is similar, but the surface lies below the $x,y$ plane.
%D  Elliptic paraboloid

If the signs are different, the surface is called a {\it
hyperbolic paraboloid}.  Its intersection with planes perpendicular
to the $z$-axis are hyperbolas asymptotic to the lines in
those planes parallel to the lines $x/a = \pm y/b$.   Its intersection
with the $x,y$-plane is just those two lines.  The surface has
a saddle point at the origin.
\outind{hyperbolic paraboloid}%
\outind{paraboloid, hyperbolic}%
\outind{saddle}%
%D  hyperbolic paraboloid

The locus of the equation $z = cxy, c\not= 0$ is also a hyperbolic
paraboloid, but rotated so it intersects the $x,y$-plane in the
$x$ and $y$ axes. 
\medskip
\centerline{\epsfbox{s3-11.ps}}
\medskip
Finally, we should note that many so called ``degenerate conics''
are loci of quadratic equations.   For example, consider
$$
    \frac{x^2}{a^2} + \frac{y^2}{b^2} - \frac{z^2}{c^2} = 0
$$
which may be solved to obtain
$$
   z = \pm c\sqrt{\frac{x^2}{a^2} + \frac{y^2}{b^2}}. 
$$

\mar{s3-12.ps}
The locus is a double cone with elliptical cross sections and
vertex at the origin.  

\subhead Generalizations \endsubhead
In general, we will want to study functions of any number of independent
variables.   For example, we may define the graph of a scalar valued
function $f$ of three independent variables to be the set of all points
in $\R^4$ of the form $(x,y,z,f(x,y,z))$.   Such an object
should be considered a three dimensional subset of $\R^4$, and it is
certainly not easy to visualize.  It is more useful to consider the
analogues of level curves for such functions.  Namely, for each
possible value $c$ attained by the function, we may consider the
locus in $\R^3$ of the equation $f(x,y,z) = c$.  This is generally
a surface called a {\it level surface\/} for the function. 
\outind{level surface}%

\mar{s3-13.ps}
\smallskip
\example{Examples}
For $f(x, y, z) = x^2 + y^2 + z^2$, the level surfaces are
concentric spheres centered at the origin if $c > 0$.   For
$c = 0$ the level `surface' is not really a surface at all; it
just consists of the point at the origin.   (What if $c < 0$?) 

For $f(x,y,z) = x^2 + y^2 - z^2$, the level surfaces are either
hyperboloids of one sheet if $c > 0$ or hyperboloids of two sheets
if $c < 0$.  (What if $c = 0$?)
\endexample

For functions of four or more variables, geometric interpretations
are even harder to come by.  If $f(x_1,x_2,\dots,x_n)$ denotes
a function of $n$ variables, the locus in $\R^n$  of the
equation $f(x_1,x_2,\dots,x_n) = c$ is called a level set,
but one doesn't ordinarily try to visualize it geometrically.  

Instead of talking about many independent variables, it is useful
to think instead of a single independent variable which is a 
{\it vector}, i.e., an element of $\R^n$ for some $n$.   In the
case  $n = 2, 3$, we usually write $\r = \lb x,y \rb$ or
$\r = \lb x,y,z \rb$ so $f(x,y)$ or $f(x,y,z)$ would be written 
simply $f(\r)$.  If $n > 3$, then one often denotes the
variables $x_1, x_2, \dots, x_n$ and denotes the vector
(i.e., element of $\R^n$) by $\x = (x_1,x_2, \dots, x_n)$.
Then $f(x_1,x_2, \dots, x_n)$ becomes simply  $f(\x)$.
The case of a function of a single real variable can be subsumed
in this formalism by allowing the case $n =1$.  That is,
we consider a scalar $x$ to be just a vector of dimension 1,
i.e, an element of $\R^1$.

When we talked about kinematics, we considered  {\it vector valued\/}
functions $\r(t)$ of a single independent variable.    Thus we see
that it makes sense to consider in general functions of a vector
variable which can also assume vector values.   We indicate this
by the notation $f:\R^n \to \R^m$.   That shall mean that the domain
of the function $f$ is a subset of $\R^n$ while the set of values is
a subset of $\R^m$.    Thus, $n = m = 1$ would yield a scalar
function of one variable, $n = 2, m = 1$ a scalar function of two
variables, and $n = 1, m = 3$ a vector valued function of one scalar
variable.   We shall have occasion to consider several other special
cases in detail.

There is one slightly non-standard aspect to the above notation.
In ordinary usage in mathematics,
``$f:\R^n \to \R^m$'' means that $\R^n$ is the entire domain of the
function $f$, whereas we are taking it to mean that the domain is some
subset.    We do this mostly to save writing since usually the
domain will be almost all of $\R^n$  or at least some significant
chunk of it.  What we want to make clear by the notation is the
dimensionality of both the independent and dependent variables.

\bigskip
% Section 1 Exercises
\input chap3.ex1
\bigskip
\nextsec{Limits and Continuity}
\head Section \sn.  Limits and Continuity \endhead

Most users of mathematics don't worry about things that might
go wrong with the functions they use to represent physical quantities.
They tend to assume that  functions  are differentiable when derivatives
are called for  (except possibly for a finite set of isolated points),
and they assume all functions which need to be integrated are continuous
so the integrals will exist.   For much of the period during which
Calculus was developed (during the 17th and 18th centuries), mathematicians
also did not bother themselves with such matters.   Unfortunately,
during the 19th century, mathematicians discovered that general
functions could behave in  unexpected and subtle ways, so they began to 
devote much more time to careful formulation of definitions and careful
proofs in analysis.   This is an aspect of mathematics which is
covered in courses in real analysis, so we won't devote much time
to it in this course.  (You may have noticed that we didn't worry
about the existence of derivatives in our discussion of
velocity and acceleration.)  However, 
for functions of several variables,
lack of rigor can be more troublesome than in
the one variable case,  so we briefly devote some attention to such
questions.
 In this section, we shall discuss the concepts
of {\it limit\/} and {\it continuity\/} for functions $f:\R^2 \to \R$.
The big step, it turns out, is going from one independent 
variable to two.
Once you understand that, going to three or more
independent variables introduces
few additional difficulties.

Let  $\r_0 = \lb x_0, y_0 \rb$ be (the position vector of) a point
in the domain of the function $f$.   We want to define the
concept to be expressed symbolically
$$
    \lim_{\r \to \r_0} f(\r) = L \qquad\text{or}\qquad 
   \lim_{(x,y)\to (x_0,y_0)} f(x,y) = L.
$$
\outind{limit of a function of several variables}%
We start with two examples which illustrate the concept and some
differences from the single variable case.

\nextex
\xdef\ExOne{\en}
\example{Example \en}  Let $f(x,y) = x^2 + 2y^2$, and consider the
nature of the graph of $f$  near the point $(1,2)$.  As
we saw in the previous section, the graph is an elliptic paraboloid,
the locus of $z = x^2 + 2y^2$.
In particular, the surface is quite smooth, and if $(x,y)$ is
a point in the domain {\it close to\/} $(1,2)$, then $f(x,y)$
will be very close to the value of the function there, $f(1,2) =
1^2 + 2(2^2) = 9$.   Thus, it makes sense to assert that
$$
 \lim_{(x,y)\to (1,2)} x^2 + 2y^2 = 9.
$$
\endexample

\mar{s3-14.ps}
In Example \en, the limit was determined simply by evaluating the
function at the desired point.  You may remember that in the single
variable case, you cannot always do that.
For example, putting $x = 0$ in  $\sin x/x$ yields the meaningless
expression $0/0$, but $\lim_{x \to 0} \sin x/x$ is known to be 1.
Usually, it requires some ingenuity to find such examples in the single
variable case, but the next example shows that fairly simple formulas
can lead to unexpected difficulties for functions of two or more
variables.

\nextex
\example{Example \en}  Let
$$
   f(x,y) = \frac{x^2 - y^2}{x^2 + y^2}\qquad\text{for } (x,y) \not= (0,0).
$$
 What does the graph of this function look like in the vicinity
of the point $(0,0)$?   (Since, $(0,0)$ is not in the domain of
the function, it does not make sense  to talk about
$f(0,0)$, but we can still seek a `limit'.)  The easiest way to
answer this question is to switch to polar coordinates.   Using
$x = r\cos\theeta, y = r\sin\theeta$, we find
$$
f(\r) = f(x,y) = \frac{r^2\cos^2\theeta - r^2\sin^2\theeta}
{r^2\cos^2\theeta + r^2\sin^2\theeta} = \cos^2\theeta - \sin^2\theeta
 = \cos 2\theeta.
$$
Thus, $f(\r) = f(x,y)$
is independent of the polar coordinate $r$ and depends only
on $\theeta$.  As $r = |\r| \to 0$ with $\theeta$ fixed, $f(\r)$
is constant, and equal to $\cos 2\theeta$, so, if  it `approaches'
a limit, that limit would have to be $\cos 2\theeta$.  
Unfortunately, $\cos 2\theeta$ varies between $-1$ and $1$, so it
does not make sense to say $f(\r)$ has a limit as $\r \to \bold 0$.
You can get some idea of what the graph looks like by studying the
level curves which are pictured in the diagram.  For each value
of $\theeta$, the function is constant, so the level curves consist
of rays emanating from the origin, as indicated.  On any such
ray, the graph is at some constant height $z$ with $z$ taking on
{\it every value\/} between $-1$ and $+1$.
\endexample

\mar{s3-15.ps}
In general, the statement
$$
  \lim_{\r \to \r_0} f(\r) = L
$$
will be taken to mean that $f(\r)$ is {\it close to\/} $L$ whenever
$\r$ is {\it close to \/} $\r_0$.   As in the case of functions
of a single scalar variable, this can be made completely precise
by the following `$\epsilon, \delta$' definition.
 
\block
For each number $\epsilon > 0$, there is a number $\delta > 0$
such that
$$
   0 < |\r - \r_0| < \delta\qquad\text{implies}\qquad
 |f(\r) - L| < \epsilon.
$$
\endblock
In this statement, $|\r - \r_0| < \delta$ asserts that the distance
from $\r$ to $\r_0$ is less than $\delta$.  Since $\delta$ is
thought of as small, the inequality  makes precise the meaning of
`$\r$ is close to $\r_0$'.   Similarly, $|f(\r) - L| < 
\epsilon$ catches the meaning of `$f(\r)$ is close to 
$L$'.  Note that we never consider the case $\r = \r_0$,
so the value of $f(\r_0)$
 is not relevant in checking the limit as $\r \to \r_0$.
(It is not even necessary that $f(\r)$  be well defined at
$\r = r_0$.)

Limits for functions of several variables behave formally much
the same as limits for functions of one variable.  Thus, you
may calculate the limit of a sum by taking the sum of the
limits, and similarly for products and quotients (except that
for quotients the limit of the denominator should not be zero).
The understanding you gained of these matters in the single
variable case should be an adequate guide to what to expect
for several variables.  If you never really understood all this
before, we won't enlighten you much here.  You will have to
wait for a course in real analysis for real understanding.

\mar{s3-16.ps}
\subhead Continuity \endsubhead
In Example \ExOne, the limit was determined simply by evaluating the
function at the point.  This is certainly not always possible
because the value of the function may be irrelevant or there
may be no 
meaningful 
way to 
attach a value.   Functions
for which it is always possible to find the limit this way
 are called
{\it continuous}.  (This is the same notion as for functions
\outind{continuous function of several variables}%
of a single scalar variable).  More precisely, we say that
$f$ is continuous at a point $\r_0$ if the point is in its domain (i.e.,
$f(\r_0)$ is defined) and
$$
  \lim_{\r \to \r_0} f(\r) = f(\r_0).
$$
Points at which this fails  are called {\it discontinuities}
or sometimes {\it singularities}.   (The latter term is also sometimes
\outind{discontinuity}%
reserved for less serious kinds of mathematical pathology.)  It
sometimes happens, that a function $f$ has a well defined limit $L$ at
a point $\r_0$ which does not happen to be in the domain of the function,
i.e., $f(\r_0)$ is not defined.  (In the single variable case,
$\sin x/ x$ at $x = 0$ is a good example.)  Then we can extend the
domain of the function to include the point $\r_0$ by defining
$f(\r_0) = L$.   Thus the original function had a discontinuity,
but it can be eliminated simply by extending the definition of the
function.  In this case, the discontinuity is called
{\it removable}.   As Example \en\ shows, there are functions
with  discontinuities
which cannot be defined away no matter what you try.  
 
  A function without discontinuities is called continuous.
Continuous functions have graphs which look reasonably 
smooth.  They don't have big holes or sudden jumps,
 but as we shall see later, they can still look pretty
bizarre.   Usually, just knowing that a function is continuous
won't be enough to make it a good candidate to represent a
physical quantity.  We shall also want to be able to take
derivatives and do the usual things one does in differential
calculus, but as you might expect, this is somewhat more involved
than it is in the single variable case. 

\bigskip
% Section 2 Exercises
\input chap3.ex2
\bigskip

\nextsec{Partial Derivatives}
\head Section \sn. Partial Derivatives \endhead 

Given a function $f$ of two or more variables, its {\it partial
derivative\/} with respect to one of the independent variables
\outind{partial derivative}%
\outind{derivative, partial}%
is what is obtained by differentiating with respect
to that variable while keeping all other variables 
constant.

\example{Example}
In thermodynamics, the function defined by
$$
   p = f(v,T) = k \frac T v
$$
expresses the pressure $p$ in terms of the
volume $v$ and the temperature $T$ in the case of an `ideal gas'.  
Here $v$ and $T$ are considered to be independent
variables, and $k$ is a constant.  ($k = nR$
where $n$ is the number of moles of the gas and $R$ is
a physical constant.)   The partial derivative with respect
\outind{thermodynamics}%
to $v$ (keeping $T$ constant) is
$$
      -k\frac T {v^2}
$$
while the partial derivative with respect to $T$  (keeping $v$
constant) is
$$
      k\frac 1 v.
$$
\endexample
     
\subhead Notation \endsubhead
One uses a variety of notations for partial derivatives.
For example, for a function $f$ of two variables,
$$\frac{\partial f}{\partial x}(x,y)\qquad\text{and}\qquad f_x(x,y)$$ 
are used to denote the partial derivative with respect to
$x$ ($y$ kept constant). 

\example{Example}
$$\align
 f(x,y) &= 2x + \sin(xy) \\
 f_x(x,y) &= 2 + \cos(xy)\, y = 2 + y\cos(xy) \\
 f_y(x,y) &= 0 + \cos(xy)\, x= x\cos(xy).
\endalign
$$
\endexample

  In some circumstances, the variable
names may change frequently in the discussion, so the partial
derivative is indicated by an numerical subscript giving the
position of the relevant variable.  Thus,
$$
    f_2(x, y, z, t)
$$
denotes the partial derivative of $f(x,y,z,t)$ with respect to
the second variable, in this case $y$.  In thermodynamics,
one may see things like
$$
    \left(\frac{\partial p}{\partial v}\right)_T
$$
which is interpreted as follows.  It is supposed
there is a functional dependence
$p = p(v, T)$ and the notation represents the partial derivative
of this function with respect to $v$ with $T$ kept constant.

It should be emphasized that just as in calculus of one variable,
it is only {\it functions\/} which can have derivatives (partial
or not).  It does not make sense to ask for the rate of change
of one variable with respect to another without assuming there is
a specific functional relation between the two.  In the many variable
case, since there are other variables which may be interrelated in
complex ways, it is specially important to get this distinction
straight.

If $f(x,y)$ describes a function of two variables, its partial
derivatives could in fact be defined directly by
$$\align
   f_x(x,y) &= \lim_{\Delta x \to 0}
            \frac{f(x + \Delta x, y) - f(x,y)}{\Delta x} \\
   f_y(x,y) &= \lim_{\Delta y \to 0}
            \frac{f(x, y + \Delta y) - f(x,y)}{\Delta y}. 
\endalign$$

\subhead Geometric Interpretation for $f:\R^2 \to \R$ \endsubhead
Let $f$ be a function with domain a subset of $\R^2$ and assuming
scalar values.  Fix a point $(x_0, y_0)$ in the domain of $f$.
We shall give geometric interpretations of $f_x(x_0, y_0)$
and $f_y(x_0, y_0)$ in terms of the graph of the function $f$.
First consider the function of $x$ given by $f(x, y_0)$
(i.e., $y$ is kept constant, and $x$ varies in the vicinity of
$x = x_0$).  The graph of $z = f(x, y_0)$ may be viewed as
the curve in which the plane $y = y_0$ intersects the 
 graph
of $f$.  It is called the sectional curve in the $x$-direction.
The partial derivative $f_x(x_0, y_0)$ is the slope of
this curve for $x = x_0$.  In other words, it is the slope of
the tangent line to the curve at the point $(x_0, y_0, f(x_0, y_0))$
on the graph.  Similarly, fixing $x = x_0$, and letting
$y$ vary leads to the sectional curve in the $y$-direction.
(It is the intersection
of the plane $x = x_0$ with the graph of the function.)  Its slope
for $y = y_0$ is the partial derivative $f_y(x_0, y_0)$.
Study the diagram to see how the two sectional curves and their
tangents at the common point $(x_0, y_0, f(x_0, y_0))$
are related to one another.  Note in particular that they lie
in two mutually perpendicular planes.

\medskip
\centerline{\epsfbox{s3-17.ps}}
\medskip
The two tangent lines to the sectional curves determine a plane
through the point $(x_0, y_0, f(x_0, y_0))$.  It is reasonable
to think of it as being {\it tangent\/} to the surface at
that point. Put $z_0 = f(x_0, y_0)$.
   From the above discussion, it is clear that the
first sectional tangent (in the $x$-direction) may be characterized
by the equations
$$
     z - z_0 = f_x(x_0,y_0)(x - x_0),\qquad y = y_0.
$$
This characterizes it as the intersection of {\it two planes}.
Similarly, the other sectional tangent (in the $y$-direction)
may be characterized by the equations
$$
    z - z_0 = f_y(x_0,y_0)(y - y_0), \qquad x = x_0.
$$ 
However, the plane characterized by the equation
\nexteqn
$$
    z - z_0 = f_x(x_0,y_0)(x - x_0) + f_y(x_0,y_0)(y - y_0)\tag\eqn
$$
contains both these lines, the first by intersecting with $y = y_0$
and the second by intersection with $x = x_0$.  
It follows that (\eqn) is the equation of the desired tangent plane.
\outind{tangent plane to a graph}%

\example{Example}
Let $f(x,y) = x^2 - y^2$.  We find the tangent plane at $(1,1, 0)$
($x_0 = 1, y_0 = 1, z_0 = 1^2 - 1^2 = 0$).
We have
$$
\align
f_x(x,y) &= 2x = 2\quad\text{at } x = 1, y = 1, \\
f_y(x,y) &= -2y = -2\quad\text{at } x = 1, y = 1.
\endalign
$$
Hence, the tangent plane has equation
$$\align
  z - 0 &= 2(x -1) + (-2)(y - 1)\\
\intertext{or}
  2x - 2y - z = 0
\endalign
$$
You should try to sketch the surface and the tangent plane.  You
may find the picture somewhat surprising.
\endexample

\bigskip
%Section 3 Exercises
\input chap3.ex3
\bigskip
\nextsec{First Order Approximation and the Gradient}
\head Section \sn.  First Order Approximation and the Gradient \endhead

Most functions cannot be calculated directly, and so one uses
{\it approximations\/} which are accurate enough for one's needs.
For example, the statement
$$
   e =  2.71828
$$
is presumably accurate to 5 decimal places, but it is certainly not
an exact equality.   (In fact, $e$ cannot be given exactly
by any finite decimal.  Do you know why?)  You may have
learned in your previous calculus course that $e^x$ may be
represented in general by an {\it infinite series\/}
$$
   e^x = 1 +  x + \frac{x^2}{2!} + \frac{x^3}{3!} + \frac{x^4}{4!} + \dots
$$
\outind{infinite series}%
In calculations, you use as many terms as necessary to get the
accuracy you need.   In general, many interesting functions
can be represented by {\it power series\/}, that is we have
$$
   f(x) = a_0 + a_1x + a_2x^2 + a_3x^3 + \dots + a_nx^n + \dots
$$
\outind{power series}%
(Do you know what $a_n$ is and how it is related to the function $f$?
Refer to the chapter on {\it Taylor series\/} in your one variable
\outind{power series}%
Calculus book.)   The simplest kind of approximation is the {\it linear
approximation\/} where one ignores all terms of degree higher than one.
\outind{linear approximation}%
\outind{approximation linear}%
The linear approximation is intimately tied up with the notion
of derivative.  We review what you learned in one variable
calculus about derivatives, approximation, and tangent lines.

Let $f:\R\to \R$ denote a function of a single variable. 
The relation between the value of the function 
$y = f(x)$ at one point and its value 
$y + \Delta y = f(x + \Delta x)$ at a nearby point is
given approximately by
$$
   f(x + \Delta x) \approx f(x) + f'(x)\Delta x.
$$
The quantity $f'(x)\Delta x$ may be interpreted geometrically
as the change in $y$ along the tangent line.  (See the
diagram.)  It is also sometimes
expressed in  {\it differential notation\/}
$$
    dy = f'(x) dx
$$
where for consistency we put $\Delta x = dx$.  Here, $dy$ is
only an approximation for the true change $\Delta y$, but one
often acts as though they were equal.  (Differentials are a wonderful
tool for doing calculations, but sometimes it requires great
ingenuity to see why the calculations really work.)
\outind{differential}%
\medskip
\centerline{\epsfbox{s3-18.ps}}
\medskip
  If we
want to be completely accurate, we should write instead
\nexteqn
\xdef\EqAp{\eqn}
$$
   f(x + \Delta x) = f(x) + f'(x)\Delta x + e(x,\Delta x)
$$  
where $e$ is an ``error term'' representing the difference between
the value of the $y$-coordinate on the graph of the function and
$y$-coordinate on the tangent line.  Since the tangent line is
very close to the graph, we
expect $e$ to be very small,
 at least if $\Delta x$ is small. 

One way to think of this is that there is an infinite expansion
in ``higher order terms'' which involve powers of $\Delta x$
$$
   f(x+ \Delta x) = f(x) + f'(x)\Delta x + \dots
$$
and the tangential approximation ignores
all terms of degree greater than one.
$e$ would  be the sum of these additional terms.
(This may have been brought up in your previous Calculus
course as part of a discussion of Taylor's formula.)

To proceed further, we need to be careful about what we mean
by $e(x,\Delta x)$ ``being very small''.   Indeed, all the incremental
quantities will be small, so we must ask ``small compared to
what?''  To answer this question, rewrite (\EqAp) by transposing
and dividing by $\Delta x$
$$
\frac{f(x + \Delta x) - f(x)}{\Delta x}
=  f'(x) + \frac{e(x,\Delta x)}{\Delta x}.
$$
Letting $\Delta x \to 0$, we see that the left hand side
approaches $f'(x)$ as a limit, so it follows that
$$
\lim_{\Delta x \to 0} \frac{e(x,\Delta x)}{\Delta x} = 0.
$$
This says that if $\Delta x$ is small, the ratio $e/\Delta x$ will
be small, i.e., that $e$ is  small even when compared
to $\Delta x$.  A simple example will illustrate this.

\nextex
\xdef\ExOne{\en}
\example{Example \en}  Let $f(x) = x^3, \, x = 2$.  Then
$$
   f(x +\Delta x) = (2 + \Delta x)^3 = 8 + 12\Delta x + 6\Delta x^2 +
        \Delta x^3.
$$
Here, $f'(x) = f'(2) = 3\cdot 2^2 = 12$.  Hence,
$f(2) + f'(2)\Delta x = 8 + 12\Delta x$, and $e(2,\Delta x) =
6\Delta x^2 + \Delta x^3$. Indeed,
$$
    \frac{e}{\Delta x} =  6\Delta x + \Delta x^2.
$$
Thus, if $\Delta x = .01$ (a fairly small number), $e/\Delta x
 = .0601$, and $e = .000601$ which is quite a bit smaller than
$\Delta x = .01$.
\endexample

The theory of linear approximation for
functions of two (or more variables) is similar, but
complicated by the fact that 
more things are
allowed to vary.  We start with an example.

\nextex
\example{Example \en}  Let $f(\r) = f(x,y) = x^2 + xy$,
and let $\r = (x, y) = (1,2)$.   
  Then,
proceeding as
in Example \ExOne,  we have
\nexteqn
\xdef\ErrEq{\eqn}
$$
\align
f(x+ \Delta x, y + \Delta y) &= f(1 + \Delta x, 2 + \Delta y) \\ 
&= (1 + \Delta x)^2 + (1 + \Delta x)
(2 + \Delta y) \\
&= 1 + 2\Delta x + \Delta x^2 + 2 + 2\Delta x + \Delta y + \Delta x \Delta y
   \\
&= 3 + 4\Delta x + \Delta y + \Delta x^2 + \Delta x\Delta y. \tag\eqn
\endalign
$$
These terms can be grouped naturally.
$f(1,2) = 3$ so the first three  terms may be written
$$
    f(1,2) + 4\Delta x + \Delta y
$$
and these constitute the linear terms or linear approximation
to $f(x + \Delta x,y + \Delta y)$.  Denote the remaining terms
$$
    e(\r, \Delta \r) =  \Delta x^2 + \Delta x\Delta y.
$$
In the one variable case, we compared $e$ to $\Delta x$, but now
we have both $\Delta x$ and $\Delta y$ to contend with.  The
way around this is to use
$ \Delta s = |\Delta \r| =
\sqrt{\Delta x^2 + \Delta y^2}$.
Thus,
$$
   \frac e{\Delta s} = \frac{\Delta x^2 + \Delta x \Delta y}{\Delta s}
    =  \frac{\Delta x}{\Delta s}\Delta x + 
     \frac{\Delta x}{\Delta s}\Delta y ,
$$
and the quantity on the right approaches zero as $\Delta s \to 0$.
(The reasoning is that since $|\Delta x|, |\Delta y| < \Delta s$,
it follows that the fraction $\Delta x/\Delta s$ has absolute
value never exceeding 1, while both $\Delta x$ and $\Delta y$
must approach 0 as $\Delta s \to 0$.)   It follows that
for $\Delta s$ small, $e$ is small even compared to $\Delta s$.
For example, let $\Delta x = .003, \Delta y = .004$.  Then, $\Delta
s = .005$, while
$e = (.003)^2 + (.003)(.004) =  0.000021$.

Note that the coefficients of $\Delta x$ and $\Delta y$ are just
the partial derivatives $f_x(1,2)$ and $f_y(1,2)$.   This is
not very surprising.    We can see why it works by calculating
$$\align
   f_x(1,2) &= \lim_{\Delta x \to 0}\frac{f(1+\Delta x, 2) - f(1,2)}
  {\Delta x} \\
\intertext{which from (\ErrEq) with $\Delta y = 0$}
&= \lim_{\Delta x \to 0}
\frac{4\Delta x + \Delta x^2}{\Delta x}\\
    &= \lim_{\Delta x \to 0}(4 + \Delta x) = 4.
\endalign
$$
A similar argument shows that
 $f_y(1,2)$ is
 the coefficient of $\Delta y$ (which
is 1).
\endexample

In general, suppose we have a function $f:\R^2 \to \R$.  Fix a
point in the domain of $f$ with position vector $\r = \lb x, y\rb$,
and consider the change in the function when we change $\r$
by $\Delta \r = \lb \Delta x, \Delta y \rb$.
It may be possible to express $f$
near  $\r$ by a linear approximation, i.e., to write
\nexteqn
$$
f(x + \Delta x, y + \Delta y) = f(x, y) + a\Delta x  + b\Delta y + e(\r, \Delta \r) \tag\eqn
$$
where
$$
      \lim_{\Delta \r\to 0}\frac{e(\r,\Delta \r)}{|\Delta \r|} = 0.
$$
(This last statement says that $e$ is small compared  to
$\Delta s = |\Delta \r|$ when the latter quantity
is small enough.)  If this is the case, it is not hard to see,
just as in the example, that
$$
\align
     a &= \frac{\partial f}{\partial x} \\    
     b &= \frac{\partial f}{\partial y} .
\endalign
$$
So (\eqn) may be rewritten
\nexteqn
\xdef\EqDiff{\eqn}
$$
f(x + \Delta x, y + \Delta y) 
= f(x, y) + f_x(x,y)\Delta x + f_y(x,y)\Delta y
+ e(\r,\Delta \r).\tag\eqn
$$
If this is so, we say that the function is {\it differentiable\/}
\outind{differentiable function of several variables}%
at the point $\r$ in its domain.  Equation (\EqDiff) may be
interpreted as follows.   The first term on the right $f(x,y)$
is the value of the function at the base point.  Added to this
is the {\it linear part of the change in the function\/}.  This
has two parts: the partial change $f_x\Delta x$ due only to
the change in $x$ and the partial change $f_y\Delta y$ due
only to the change in $y$.  Each partial change is appropriately
the rate of change for that variable times the change in the
variable.  Finally, added to this is the discrepancy $e$
resulting from ignoring all but the linear terms. 

Equation (\EqDiff) also has a fairly simple geometric interpretation.
Recall from the previous section that the tangent plane
(determined by the sectional tangents) at
the point $(x_0, y_0, z_0 = f(x_0,y_0))$  
in the graph of $f$
 has equation
$$
   z - z_0 = f_x(x_0, y_0)(x - x_0) + f_y(x_0, y_0)(y - y_0).
$$
Except for a change in notation this is exactly what
we have for the middle two terms on the right of (\EqDiff).
We have just changed the names of the coordinates of
the base point from $(x_0, y_0)$ to $(x, y)$, and the increments
in the variables
from $(x - x_0, y - y_0)$ to
 $(\Delta x, \Delta y)$.
Hence, $f$ is differentiable at $(x, y)$ exactly when 
the tangent plane  at $(x, y, f(x, y))$
is a good approximation to the graph
\outind{tangent plane and differentiability}%
of the function, at least if we stay close to the point of tangency.
\smallskip
\emar{s3-19.ps}{-100}
\smallskip
\subhead The Gradient \endsubhead
The {\it gradient\/} of a function $f:\R^2 \to \R$ is defined to be the vector
\outind{gradient}%
with components $\lb \dfrac{\partial f}{\partial x}, \dfrac{\partial f}{
\partial y} \rb$.   It is denoted $\nabla f$  (pronounced
``del f'') or $\grad f$.    
\outind{$\noexpand\nabla f$}%

\example{Example}
Let  $f(x,y) = x^2 + xy + 2y^2$.  Then $\nabla f = \lb
 2x + y, x + 4y \rb$.  It may also be expressed using the unit
vectors $\i$ and $\j$ by
$\nabla f(x,y) = (2x + y)\i + (x + 4y)\j$.
\endexample

Notice that the gradient is actually a function of position $(x,y)$.

The gradient may be used to simplify the expression for the
linear approximation.  Namely,
$f_x\Delta x + f_y\Delta y$ can be
rewritten as $\nabla f\cdot \Delta \r$.  Hence, we can
write the differentiability condition purely in vector notation
$$
  f(\r + \Delta \r) = f(\r) + \nabla f\cdot \Delta \r + e(\r,\Delta \r)
$$
where $\dfrac{e(\r,\Delta \r)}{|\Delta \r|} \to 0$  as $\Delta\r \to 0$.
If you look carefully, this looks quite a bit like the corresponding
equation in the single variable case with $\nabla f$ playing the
role of the ordinary derivative.  For this reason, it makes sense
to think of $\nabla f$ as a higher dimensional derivative.

Note that much of the discussion is this section could have been
done for functions $f:\R^3 \to \R$ of three variables (or indeed
for functions of any number of variables).  Thus, for a function
of three variables given by $f(\r) = f(x,y,z)$, the gradient
$\nabla f = \dfrac{\partial f}{\partial x} \i
+
\dfrac{\partial f}{\partial y} \j
+ \dfrac{\partial f}{\partial y} \k$.
Also the differentiability condition looks the same in vector
notation:
$$
    f(\r + \Delta \r) = f(\r) + \nabla f\cdot \Delta \r + e(\r,\Delta \r)
$$
where   $e(\r,\Delta \r)/|\Delta \r| \to 0$ as $|\Delta \r| \to 0$.
However, when written out in terms of components, this becomes
$$
    f(x+ \Delta x,y + \Delta y,z + \Delta z) = f(x,y,z) +
           f_x\Delta x  + f_y\Delta y
      + f_z\Delta z  + e.
$$
The geometric meaning of all this is much less clear since the
number of dimensions is too high for clear visualization.   Hence,
we usually develop the theory in the two variable case, and then
proceed by analogy in higher dimensions.  Fortunately, the
notation need not change much if we consistently use vectors.

\subhead `$O$' and `$o$' notation \endsubhead
In analysis and its applications, one is often interested in the
general behavior of functions rather than their precise forms.
Thus, we don't really care how to express the error term $e(\r, \r_0)$
exactly as a formula, but we do know something about how fast it
approaches zero.  Similarly, the most important thing about the
exponential function $e^x$ is not its exact values, but the fact that
it gets large very fast as $x \to \infty$.    The term
``order of magnitude'' is often used in these contexts.  There are
\outind{order of magnitude}%
\outind{$O$-notation}%
\outind{$o$-notation}%
two common notations used by scientists and mathematicians in this
context.    We would say that a quantity $e$ is `$O(\Delta s)$'
 as $\Delta s \to 0$ if the ratio $e/\Delta s$ stays bounded.
That means they have roughly the same order of magnitude.  We say
that $e$ is `$o(\Delta s)$' if the ratio $e/\Delta s$ goes to
zero.   That means that $e$ is an order of magnitude (or more) smaller
than  $\Delta s$.  `$o$' is stronger than `$O$' in that the former
implies the latter, but not necessarily vice versa.

  With this terminology, we could express the differentiability
condition
$$
    f(\r + \Delta \r) = f(\r) + \nabla f\cdot \Delta \r + o(|\Delta \r|).
$$
You might also see the following
$$
    f(\r + \Delta \r) = f(\r) + \nabla f\cdot \Delta\r + O(|\Delta \r|^2).
$$
This assumes more information about the error than the previous
formula.  It asserts that the error behaves like 
the square $|\Delta \r|^2$  (or better) whereas the previous
statement is not that explicit.  For almost all interesting functions
the `$O(|\Delta \r|^2)$' is valid, but mathematicians, always
wanting to use the simplest hypotheses, usually develop
the subject using the less restrictive `$o$' estimate. 


\subhead Differential Notation \endsubhead
Let $f:\R^2 \to \R$ denote a function of two variables, and fix
a point $\r$ in its domain.
As in the single variable case, we
write
\nexteqn
\xdef\TotDiff{\eqn}
$$
    dz = \nabla f\cdot d\r = 
\frac{\partial f}{\partial x}dx + \frac{\partial f}{\partial y}
dy\tag\eqn
$$
(where we put $\Delta x = dx, \Delta y = dy$, and $\Delta \r = d\r$.)
This is the change in  
 $z$ {\it in the tangent plane\/}
\outind{tangent plane}%
 corresponding to a
change $d\r = \Delta \r$.  
It should  be distinguished from the change 
$$
    \Delta z = \nabla f\cdot \Delta \r  + e.
$$ 
in the function itself. 
The expression on the right of (\TotDiff)
 is often called the {\it total
differential} of the function.   As in the one variable case, one
\outind{differential, total}%
\outind{total differential}%
can use differentials for ``quick and dirty'' estimates.  What we
are doing in essence is assuming all functions are linear, and as
long as all changes are small, this is not a bad assumption.
 
\nexteqn
\example{Example \en}
The pressure of an ideal gas is given in terms of the volume and
Temperature by the relation
$$
   p = p(v,T) = k\frac Tv
$$
where $k$ is a constant.  Suppose $v = 10, T = 300$ and both
 $v$ and $T$ increase by 
1\%.  What is the change in $p$?   To get an approximate
answer to this question, we calculate the total differential
$$
  dp = \frac{\partial p}{\partial v}dv +
       \frac{\partial p}{\partial T}dT
     = -k\frac T{v^2}dv + k\frac 1v dT.
$$
Putting $v = 10, dv = .1,\, T = 300, dT = 3$, we
get
$$ 
 dp = -k\frac {300}{10^2}.1 + k\frac 1{10} 3 = 0
$$
so to a first approximation, there is no change in $p$.
(The actual values of $v$ and $T$ were not relevant.
Can you see why?  Hint: Calculate $dp/p$ in general.)
\endexample

Calculations with differentials work just as well for functions
of any number of variables.   They amount to a use of the
linear approximation.  The only difficulty is that one can't
easily visualize things in terms of ``tangent planes'' since
the number of dimensions is too large.
\bigskip
%Section 4 Exercises
\input chap3.ex4
\bigskip
\nextsec{The Directional Derivative}
\head Section \sn.  The Directional Derivative \endhead

Consider a function $f:\R^2 \to \R$.   In the following discussion,
refer to the diagram where we have sketched in some of the contour
curves of the function.   At some point $\r$ in the domain
of the function, pick a direction, and draw a ray emanating from
the point $\r$ in the indicated direction.  We want to consider
the rate of change of the function in that direction.  This
is called the {\it directional derivative\/} in the desired
\outind{directional derivative}%
direction.   
   You can
think of it roughly as the rate at which you cross level curves
 as you move away from the point in the indicated direction.

\emar{s3-20.ps}{-300}
Unfortunately, there is no standard notation
for the directional derivative.  
 One common notation is as follows.  Specify
the direction by an appropriate {\it unit vector\/}
$\u$.   The directional derivative in the direction $\u$ is
then denoted
$$
  \left. \frac{df}{ds}\right |_{\u}.
$$
You already know about two cases.  If
the direction is that of the unit vector $\i$ (parallel to the
$x$-axis), the directional derivative is the partial
derivative $f_x(\r)$.   Similarly, if the direction is that
of $\j$, the directional derivative is the partial derivative
$f_y(\r)$.   It turns out that the directional derivative in
any direction can be expressed in terms of the partial derivatives.
To see this, let $\Delta\r = \Delta s\,\u$ be a displacement 
through distance $\Delta s$ in the direction
$\u$.  Then, if the function is differentiable at $\r$,
we have
$$\align
  f(\r + \Delta\r)  &=
      f(\r) + \nabla f(\r)\cdot\Delta\r + e \\
   &=
      f(\r) + \nabla f(\r)\cdot(\Delta s\,\u) + e 
\endalign
$$
so 
$$
\frac{f(\r + \Delta\r) - f(\r)}{\Delta s}
 = \nabla f(\r)\cdot\u + \frac e{\Delta s}.
$$
However, by hypothesis, $e/\Delta s \to 0$, so
$$
  \lim_{\Delta s\to 0}
\frac{f(\r + \Delta s\,\u) - f(\r_0)}{\Delta s}
=  \nabla f(\r_0)\cdot\u.
$$
The directional derivative is  the limit
on the left, so we obtain
$$
   \left.\frac{df}{ds}\right |_{\u} = \nabla f(\r)\cdot \u.
$$
Note that the directional derivative depends on both the
point $\r$ at which it is calculated and the direction in which
it is calculated.  Some of this may be suppressed in the notation
if it is otherwise clear,
so you may see for example
$df/ds = \nabla f\cdot\u$.

\nextex
\example{Example \en}  A climber ascends a mountain where the elevation
in feet
is given by $z = f(x,y) = 5000 - x^2 - 2y^2$.  $x$ and $y$ refer
to the coordinates (measured in feet
from the summit)  of a point on a {\it flat map\/} of the
mountain.
  Most of the discussion which follows refers
to calculations involving that map.   Suppose the climber
finds herself at the point with map coordinates $(20,10)$ and
wishes to move in the direction (on the map) of the unit
vector $(\frac 35,\frac 45)$ (a bit north of northeast).  How
fast will she descend?

\mar{s3-21.ps}
To solve this problem, we calculate the directional derivative
in the indicated direction.  First,
$$
 \nabla f = \lb -2x, -4y \rb = \lb -40, -40 \rb\qquad\text{at } (20,10).
$$
Hence, in the indicated direction
$$
   \frac{df}{ds} = \lb -40, -40 \rb\cdot\lb \frac 35, \frac 45 \rb
            = \frac{-120 -160}5 = -56.
$$
Thus she descends 56 feet for each foot she moves horizontally (i.e.,
on the map).
(She had better be using a rope!)  Notice
that the answer is negative which accords with the fact that the
climber is descending.

It is worth spending some time thinking about what this means on
the actual surface of the mountain, i.e., the graph of the
function in $\R^3$.   The point on the graph would be
that with coordinates $(20,10,5000 - 20^2 - 2\cdot10^2) = (20,10,4400)$.
{\it Moving on the mountain\/}, 
if the climber moves $\Delta s$ units horizontally
(i.e., on the map), she will move $\sqrt{\Delta s^2 + 56^2
\Delta s^2} = \Delta s \sqrt{3137}$ in space. 
You should make sure you visualize all of this.  In particular,
try to understand the relation between the unit vector
$\u$ in the plane, and the corresponding displacement vector
in space.   Can you find a vector in space which is
tangent to the graph of the function and which projects on
$\u$?   Can you find a unit vector in the same direction?
(The answer to the first question is $\lb \frac 35, \frac 45,-56
\rb$.)
\endexample

\subhead Significance of the Gradient \endsubhead
Fix a point $\r$ in the domain of the function $f$, and consider
the directional derivative
$$
     \left. \frac {df}{ds}\right|_{\u} = \nabla f\cdot\u
$$
\outind{gradient, significance of}%
as a function of $\u$.  Assume $\nabla f \ne \bold 0$
at $\r$.  (Otherwise, the directional derivative is always
zero.)  The directional derivative is 0 if 
$\u$ is perpendicular to $\nabla f$.  It attains is maximum positive
value if $\u$ points in the direction of $\nabla f$, and it
attains is minimum (most negative) value if $\u$ points in the
direction opposite to $\nabla f$, (i.e., $-\nabla f$.)
Finally, if $\u$ points the same way as $\nabla f$, the directional
derivative is
$$
   \frac{df}{ds} = \nabla f\cdot\u = |\nabla f||\u| = |\nabla f|.
$$

\mar{s3-22.ps}
The upshot of this is that
\roster
\item  the direction of the gradient is that in which the
directional derivative is as large as possible;
\item  the magnitude of the gradient is the directional derivative
in that direction
\endroster
\outind{directional derivative}%

\example{Example \en, {\rm revisited}}
Consider the same climber on the same mountain.  In which direction
should she move (on her map) to go down hill as fast as
possible?  By the above analysis, this is opposite
to the direction of the
gradient, i.e., the direction of  $-\nabla f(1,2) = \lb 40, 40 \rb$.
Directions are often given by unit vectors, so we might normalize
this to $\u = (1/\sqrt 2)\lb 1, 1 \rb$.
Note that the question of finding a vector on the surface of
the mountain pointing down hill is somewhat different.  Can
you solve that problem? (The answer is $\dfrac 1{\sqrt{2}}\lb 1, 1, -80 \rb$.
This is not a unit vector, but you can get a unit vector
in the same direction---should you need one---by dividing it by its length.)
\endexample
\bigskip
%Section 5 Exercises
\input chap3.ex5
\bigskip
\nextsec{Criteria for Differentiability}
\head Section \sn. Criteria for Differentiability \endhead


We need a simple way to tell if a function $f:\R^n \to \R$ is
differentiable.  Following the philosophy enunciated previously,
we concentrate on the case $n = 2$, and proceed by analogy in
higher dimensional cases.
\outind{differentiable function of several variables}%

\nextthm
\proclaim{Theorem \cn.\tn} Let $f:\R^2 \to \R$.  Suppose the
partial derivatives $\dfrac{\partial f}{\partial x}$ and
$\dfrac{\partial f}{\partial y}$ exist and are continuous
functions in the vicinity of the point $\r$.   Then $f$ is
differentiable at $\r$.
\endproclaim

How would you generalize this result to apply to functions of
three or more variables?

\example{Examples}
If we take $f(x,y) = x^2 + 3 y^2$, then $f_x(x,y) = 2x$,
and $f_x(x,y) = 6y$. These are certainly continuous for
all points $(x,y)$ in $\R^2$.  Hence, the theorem assures us that
$f$ will be differentiable at every point in $\R^2$.
The graph of $f$ is an elliptic paraboloid,
and it is very smooth.  At each point of the graph, we expect the
tangent plane to be a good approximation to the graph.

\mar{s3-23.ps}
On the other hand, take  $f(x,y) = \sqrt{x^2 + y^2}$.   Then
$f_x(x,y) = x/\sqrt{x^2 + y^2}$ and $f_y(x,y) = y/\sqrt{x^2 + y^2}$.
Both of these fail to be continuous at $(0,0)$ although the
function $f$ is defined there and is even continuous.   Hence,
the theorem will not insure that $f$ is differentiable at
$(0,0)$.  In fact, the graph of $f$ is a right circular cone
with its vertex at the origin, and it is clear that there is
no well defined tangent plane at the vertex.
\endexample

There is a hierarchy of degrees of ``smoothness'' for functions.
The lowest level is continuity, and the next level is
differentiability.   A function can be continuous without
being differentiable.   We saw an example of this in the
function defined by $f(x,y) = \sqrt{x^2 + y^2}$.  
 However, a differentiable function
is necessarily continuous.   (See the exercises.)

%Ex   Let $f:\R^2 \to \R$.   Show that if $f$ is differentiable
%Ex at $\r$ then $f$ is continuous at $\r$.   Hint:  The definition
%Ex of continuity at $\r$ can be rewritten
%Ex $$
%Ex    \lim_{\Delta\r \to 0} f(\r + \Delta r) = f(\r).
%Ex $$
%Ex Use the differentiability condition
%Ex $$
%Ex      f(\r +\Delta\r) = f(\r) + \nabla f(\r)\cdot \Delta\r + e(\r,\Delta\r)
%Ex $$
%Ex to verify the previous statement.   What happens to $e$ as $\Delta\r \to 0$?

Continuity of partial derivatives provides a still higher level
of smoothness.  Such functions are often called $\Cal C^1$
\outind{$\noexpand\Cal C^1$ functions}%
functions.  The theorem tells us that such functions are
differentiable.  However, if the partial derivatives are not
continuous,
we cannot necessarily conclude
that the function is {\it not\/} differentiable.  The
theorem just does not apply.   In fact there are some
not too bizarre examples in which the partials are not continuous
but where the function is  differentiable, i.e., there is a well
defined tangent plane which is a good approximation to the graph.

\subhead Proof of the Theorem \endsubhead
While it is not essential that you understand how the theorem
is proved, you might find it enlightening.   The proof makes
extensive use of the Mean Value Theorem, which you probably
saw in your previous Calculus course.  (See also {\it Edwards and
Penney\/}, 3rd Edition, Section 4.3.)  The Mean Value Theorem
may be stated as follows.  Suppose $f$ is a function of a single
variable
which is defined and continuous for $a \le x \le b$ and which
is differentiable for $a < x < b$.  Then there is a point
$x_1$ with $a < x_1 < b$ such that
$$
     f(b) -  f(a) =  f'(x_1)(b - a).
$$
If we substitute $x$ for $a$ and $\Delta x$
for $b - a$, we could also write this
$$
    f(x + \Delta x) - f(x) = f'(x_1)\Delta x.
$$
The quantity on the right looks like the change in 
the linear approximation
except that the derivative is evaluated at $x_1$ rather than at
$x$.   Also the equation is an exact equality rather than an approximation.
This form of the Mean Value Theorem is better for our purposes because
although in the previous analysis we had $\Delta x > 0$, i.e.,
$a < b$, the Mean Value Theorem
 is also true for $\Delta x < 0$.  (Just interchange the
roles of $a$ and $b$.)   In this form, we would say simply
that $x_1$
 lies {\it between\/} $x$ and $x + \Delta x$ so we don't have to
commit ourselves about the sign of $\Delta x$.

To prove the theorem, consider the difference
$f(x + \Delta x, y + \Delta y) - f(x,y)$ which we want to
    relate to  $f_x(x,y)\Delta x + f_y(x,y)\Delta y$.
We have
$$\align
f(x + \Delta x, y + \Delta y) - f(x,y) &= 
f(x + \Delta x, y + \Delta y) - f(x,y+ \Delta y) \\ 
&+f(x, y + \Delta y) - f(x,y). 
\endalign
$$
Consider the first difference on the right as a function only of the
first coordinate
(with the second coordinate fixed at $y + \Delta y$.)
By the Mean Value Theorem, there is an $x_1$ between $x$ and $x + \Delta x$
such that
$$
f(x + \Delta x, y + \Delta y) - f(x,y+ \Delta y) =
 f_x(x_1, y + \Delta y)\Delta x.
$$
Consider the second difference 
$f(x, y + \Delta y) - f(x,y)$ as a function of the second variable,
and apply the Mean Value Theorem again.  There is a $y_1$ between
$y$ and $y + \Delta y$ such that
$$
f(x, y + \Delta y) - f(x,y) = f_y(x, y_1) \Delta y.
$$
Thus,
$$
f(x + \Delta x, y + \Delta y) - f(x,y) = 
 f_x(x_1, y + \Delta y)\Delta x +
f_y(x, y_1) \Delta y,
$$
so
$$\align
e &= 
f(x + \Delta x, y + \Delta y) - f(x,y) 
- f_x(x,y)\Delta x - f_y(x,y)\Delta y \\
&= 
 f_x(x_1, y + \Delta y)\Delta x +
f_y(x, y_1) \Delta y 
- f_x(x,y)\Delta x - f_y(x,y)\Delta y \\
&= (f_x(x_1,y + \Delta y) - f_x(x,y))\Delta x + (f_y(x, y_1) - f_y(x,y))
\Delta y.
\endalign
$$
Hence,
\nexteqn
$$
\frac e{\Delta s}
 =
 (f_x(x_1,y + \Delta y) - f_x(x,y))\frac{\Delta x}{\Delta s}
 + (f_y(x, y_1) - f_y(x,y))\frac{\Delta y}{\Delta s}.\tag\eqn
$$
Now let $\Delta s \to 0$.  Since $|\Delta x| \le \Delta s$,
it follows that $\Delta x/\Delta s$ has absolute value at most
1, and a similar argument applies to $\Delta y/\Delta s$.
In addition, as $\Delta s \to 0$, so also
$\Delta x\to 0$ and $\Delta y \to 0$.   Since $x_1$
is between $x$ and $x + \Delta x$,
it follows that $x_1 \to x$.  Similarly, since
$y_1$ lies between
$y$ and $y + \Delta y$, it follows that $y_1 \to y$.   Hence,
since $f_x$ and $f_y$ are continuous functions,
$$\align
 \lim_{\Delta s \to 0} f_x(x_1, y +\Delta y) &= f_x(x,y), \\
 \lim_{\Delta s \to 0} f_y(x, y_1) &= f_y(x,y).
\endalign$$
However, 
this implies that the expressions in parentheses on the right of
(\eqn) both approach zero.   It follows that
$$
 \lim_{\Delta s \to 0}\frac e{\Delta s} = 0
$$
as required.

\subhead Domains of Differentiable Functions \endsubhead
So far we haven't been very explicit about the domain of
a function $f:\R^n \to \R$ except to say that it is some
subset of $\R^n$.   A moment's thought will convince you
that to  do differential
calculus, there will have to be some restrtiction
on possible domains.  For
example, suppose $n = 2$ so $f(x,y)$ gives a function of
two variables.  If the domain were the subset of $\R^2$
which is the locus of the equation $y = x$, it would not
make much sense to try to talk about the partial derivative
$\partial f/\partial x$ which is supposed to be the
derivative of $f$ {\it with $y$ kept constant}.   If
$y = x$, we can't vary $x$ without also varying $y$.
(The same would apply if there was any algebraic
relation between $x$ and $y$ and the domain were some
curve in $\R^2$.)
	In order to make sense of partial derivatives and
related concepts, the domain must be `fat enough', i.e.,
the variables must really be independent.
However, the derivatives at a point $\r$ depend only on values of the
variables near to the point, not on the entire domain of the
function.   Hence, the domain need only be `fat' in the
immediate vicinity of a point at which we want to take 
derivatives.

\mar{s3-24.ps}
	To make this precise, we introduce some new
concepts.  We concentrate on the case of $\R^2$.   The
generalization to $\R^3$ and beyond is straightforward.
A set $D$ in $\R^2$ is said to be an {\it open set\/}
\outind{open set}%
if it has the following property.   If a point is in
$D$ then there is an entire {\it disk\/} of some radius
\outind{disk}%
centered at the point which is contained in $D$.  (A
disk is a circle including the circumference and what is
contained inside it.)  We can state this symbolically as
follows.   If $\r_0$ is a point in $D$, then there is
a number $\delta > 0$ such that all points $\r$
satisfying $|\r - \r_0| \le \delta$ are also in $D$.
	
\example{Examples}  The entire plane $\R^2$ is certainly
open.  

Also, the set $D$ obtained by leaving out any
one point is open.

The rectangular region consisting of all points $(x,y)$
such that  $a < x < b, c < y < d$ is open, but the
region defined instead by
$a \le x \le b, c\le y \le d$ is not open.   The points
on the perimeter of the rectangle in the latter case
are in the set, but they don't have the desired property
since any disk centered at such a point will necessarily
contain points not in the rectangle.

\medskip
\centerline{\epsfbox{s3-25.ps}}
\medskip
The set of all points inside a circle but not on its circumference
is open, but the set of points in a circle or on its circumference
is not open.   The former is often called an {\it open disk},
and the latter is called a {\it closed disk}.

There are many other examples.
\endexample

There are a couple of related concepts.   If $D$ is a subset of
$\R^2$, a point is said to be on its {\it boundary\/} if
every disk centered at the point has some points in the set $D$
{\it and\/} some points not in the set $D$.  For example,
\outind{boundary}%
the perimeter of
a rectangle or the circumference of a disk consists of boundary
points. A point in $D$ is said to be an interior point if it
is not on the boundary, i.e., there is some disk centered at
the point consisting only of points of $D$.   The interior of
a set is always open.  A set is open if it consists only of
interior points.   
Finally, we say a set is {\it closed\/} if it contains all its
\outind{closed set}%
boundary points.
\medskip
\centerline{\epsfbox{s3-26.ps}}
\medskip
Generally, we only want to take derivatives at interior
points of the domain of a function because at such points we
can move in all possible independent directions, at least if
we stay sufficiently close to the point.   One way to insure
this is to assume that the domain of a function $f$ is
always an open set.   However, there are times when
we want to include the boundary of the set in the domain.  At
such points, we may not be able to take derivatives.  However,
we can sometimes do something like that if we are careful.
For example, for points on the perimeter of a rectangle we
can take ``one sided derivatives'' by allowing variation in
directions which point into the rectangle.
\smallskip
\mar{s3-27.ps}
\bigskip
%Section 6 Exercises
\input chap3.ex6
\bigskip
\nextsec{The Chain Rule}
\head Section \sn. The Chain Rule \endhead
%Section 7

The chain rule for functions of a single variable tells us how to
find the derivative of a function of a function, i.e., a composite
function.   Thus, if  $y = f(x)$ and $x = g(t)$, then
$$
     \frac d{dt}(f(g(t)) = \frac {df}{dx}(x)\frac{dg}{dt}(t)\qquad
\text{with } x = g(t).
$$
  The generalization to higher dimensions
is quite straightforward, at least if we use vector notation, but
its elaboration is terms of components can look pretty involved.
Suppose $z = f(\r)$ describes a differentiable
 function $f:\R^n \to \R$ and
$\r = \g(t)$ a vector valued differentiable 
function $\g:\R \to \R^n$.   (We shall concentrate on the two
cases $n = 2$ and $n = 3$, but it all works just as well for
any 
$n$.)   Then
$z = f(\g(t))$ describes the composite function $f\circ\g:
\R \to \R$ which is just a scalar function of a single variable.
The multidimensional chain rule asserts
\nexteqn
\xdef\ChR{\eqn}
$$
     \frac d{dt}(f(\g(t)) = \nabla f(\r)\cdot \frac{d\g}{dt}\qquad
\text{with } \r = \g(t).\tag\eqn
$$
\outind{chain rule}%
Note that the gradient $\nabla f$ plays the role that the derivative
plays in the single variable case.

Formula (\ChR) looks quite simple in vector form, but it becomes
more elaborate if we express things in terms of component functions.
Let $h(t) = f(\g(t))$ denote the composite function.  Then, for
$n = 2$, we have $\nabla f = \lb \partial f/\partial x,
\partial f/\partial y \rb$ and $d\g/dt = \lb
dx/dt, dy/dt \rb$.   Thus, the chain rule becomes
\nexteqn
\xdef\ChRTwo{\eqn}
$$
   \frac{dh}{dt} =  \frac{\partial f}{\partial x}\frac{dx}{dt}
       +
    \frac{\partial f}{\partial y}\frac{dy}{dt}.\tag\eqn
$$
Similarly, for $n = 3$, the chain rule becomes
\nexteqn
\xdef\ChRThree{\eqn}
$$
   \frac{dh}{dt} =  \frac{\partial f}{\partial x}\frac{dx}{dt}
       +
    \frac{\partial f}{\partial y}\frac{dy}{dt}
+    \frac{\partial f}{\partial z}\frac{dz}{dt}.\tag\eqn
$$

\nextex
\example{Example \en}
Let w = $f(x,y,z) = e^{xy + z}, \, x = t, y = t^2, z = t^3$.
(Thus, $\g(t) = \lb t, t^2, t^3 \rb$.  Then
$$\align
\frac{\partial f}{\partial x} &= e^{xy + z}y = e^{2t^3}t^2\\
\frac{\partial f}{\partial y} &= e^{xy + z}x = e^{2t^3}2t\\
\frac{\partial f}{\partial z} &= e^{xy + z} = e^{2t^3},
\endalign
$$
and
$$\align
 \frac{dx}{dt} &= 1 \\
 \frac{dy}{dt} &= 2t \\
 \frac{dz}{dt} &= 3t^2. 
\endalign $$
 Putting these in formula (\ChRThree) yields
$$
 \frac{dh}{dt} = e^{2t^3}t^2 + 
 e^{2t^3}t\cdot2t + 
 e^{2t^3}3t^2  = e^{2t^3}(6t).
$$ 
There are a couple of things to notice in the example. First, in
principle,
the {\it intermediate\/} variables $x, y, z$ should be expressed in
terms of the ultimate independent variable $t$.  Otherwise,
the answer might be considered incomplete.  Secondly, the derivative
could have been calculated by first making the substitutions, and
then taking the derivative.
$$
\align
     w = h(t) &= e^{t\,t^2 + t^3} = e^{2t^3} \\
    \frac{dh}{dt} &= e^{2t^3}(2\cdot 3t^2) = e^{3t^3}(6t^2).
\endalign
$$
Here we only needed to use the single variable chain rule
(to calculate the derivative of $e^u$ with $u = 2t^3$,)  and
the calculation was much simpler than that using the multidimensional
chain rule.   This is almost always the case.  About the only
exception would be that in which we happened to know the
partial derivatives $\partial f/\partial x, \partial f/\partial y$,
and $\partial f/\partial z$, but we did not know the function
$f$ explicitly.   In fact, unlike the single variable chain rule,
the multidimensional chain rule is a tool for theoretical analysis
rather than an aid in calculating derivatives.  In that role,
it amply justifies itself. 
\endexample

\subhead Proof of the Chain Rule \endsubhead
\demo{}
To prove the chain rule, we start with the differentiability
condition for $f$.
$$
f(\r + \Delta\r) = f(\r) + \nabla f(\r)\cdot\Delta\r
 + e(\r,\Delta \r)
$$
where $e/|\Delta\r| \to 0$ as $|\Delta\r| \to 0$.  
Hence,
\nexteqn
\xdef\ZZZ{\eqn}
$$
\Delta w =
  f(\r + \Delta\r) - f(\r) = \nabla f(\r)\cdot\Delta r
 + e(\r,\Delta \r), \tag\eqn
$$
and dividing by $\Delta t$ yields
$$
 \frac{\Delta w}{\Delta t} 
   = \nabla f(\r)\cdot \frac{\Delta\r}{\Delta t}
        + \frac e{\Delta t}.
$$
Now let $\Delta t \to 0$.  On the left, the limit is $dw/dt = dh/dt$.
On the right, we have
$$
 \nabla f\cdot \lim_{\Delta t\to 0}\frac{\Delta\r}{\Delta t} 
= \nabla f\cdot \frac{d\r}{dt}
$$
which is what we want,
so the rest of the argument amounts to showing that the
additional term $e/\Delta t$ goes to 0 as $\Delta t \to 0$. 

 We need to distinguish
two cases.  For a given $\Delta t$, we may have $\Delta \r = \bold 0$.
In that case, $\Delta w = 0$, and it also follows from
(\ZZZ) that $e = 0$.
Otherwise, if $\Delta \r \not= \bold 0$,  write
\nexteqn
\xdef\YYY{\eqn}
$$
 \frac e{|\Delta t|} = \frac e{|\Delta\r|}\frac{|\Delta\r|}{|\Delta t|}.
\tag\eqn
$$
Now, let $\Delta t \to 0$, but first restrict attention just to those
$\Delta t$ for which $\Delta \r = \bold 0$.   For those
$\Delta t$, we have, as noted above,  $e = 0$, so we have
trivially  $e/\Delta t \to 0$.
Next, let $\Delta t \to 0$, but restrict attention  instead just to
those $\Delta t$ for which $\Delta \r \not = \bold 0$.
Since  $|\Delta\r/\Delta t| \to |d\r/dt|$, it follows
that $|\Delta\r| \to
0$ in these cases. 
By assumption
$e/|\Delta\r| \to 0$ generally, so the same is true if we
restrict attention to those $|\Delta\r| \ne 0$ obtained
from  non-zero $\Delta t$ going to zero.
Thus, equation (\YYY) tells us that $e/|\Delta t|
\to 0$ also in the second case.

It should be noted that this is exactly the same as the usual proof of
the single variable chain rule except for modifications necessary due
to the fact that some of the arguments are vectors rather than
scalars. 
\enddemo

\subhead Geometric Interpretation of the Chain Rule \endsubhead
For variety, we consider the case of a function $f:\R^3 \to \R$
composed with a function $\g:\R \to \R^3$.   You might think of
the function $f(\r)$ giving the temperature $w$ at the point with
position vector $\r$.  Then the {\it level surfaces\/} of the
function would be called isotherms, surfaces of constant temperature.
As before $\r = \g(t)$ would be a parametric representation of
a curve in $\R^3$ and we could think of it as the path of a
particle moving in space.  The derivative $d\r/dt = \g'(t)$
would be the velocity vector at time $t$.  Then the chain rule
could be written
$$
   \frac{dw}{dt} = \nabla f(\r)\cdot \frac{d\r}{dt}
\qquad  \r = \g(t),
$$
\outind{chain rule}%
and it would say that the rate of change of temperature $w$
experienced by the particle as it moves through the point
with position vector $\r$ would be the gradient of the temperature
function $f$ at $\r$ dotted with the velocity vector at that
point of the curve.  Of course, you could think of the function
$w$ as giving any other quantity which might be interesting
in the problem you are studying.

\emar{s3-28.ps}{-50}
Note that the formula for the directional derivative
$$
   \frac{df}{ds} = \nabla f(\r)\cdot \u
$$
is a {\it special case\/} of the chain rule.  Namely, if the
particle moves in such a way that the speed $ds/dt = 1$, then
the velocity vector $\u = d\r/dt$ will be a unit vector, and
we may identify $s$ with $t$, i.e., ``distance'' with ``time''.
  
One important consequence
of the chain rule is that at any point $\r$,
 {\it the gradient\/} $\nabla f(\r)$ (provided it is not zero) {\it  is 
perpendicular to the level surface through\/} $\r$. 
For, suppose a curve given by $\r = \g(t)$ is contained in
the level surface 
$$
   f(\r) = c.
$$
Since the derivative of a constant is zero, the chain rule tells
us
$$ 
   \nabla f\cdot\frac{d\r}{dt} = 0
$$
so $\nabla f$ is perpendicular to $d\r/dt$.
On the other hand, $d\r/dt$ is tangent to the curve, so it is
also tangent to the surface.  For any reasonable
surface, we  
can manage to get {\it every possible tangent vector\/} to the surface
by a suitable choice of a curve lying in the surface.  Hence,
it follows that the gradient is perpendicular to the surface.
\medskip
\centerline{\epsfbox{s3-29.ps}}
\medskip
(The realizability of all tangents  to level surface is
a bit subtle, and we shall study it more closely in a later
section.  For now, it suffices to say that for reasonable surfaces,
there is no problem, and the gradient is always normal (perpendicular)
to the level surface.)
\outind{tangent plane to a level surface}%
\outind{gradient normal to level surface}%

\nextex
\xdef\GravEx{\en}
\example{Example \en}
Let $f(\r) = 1/|\r| = 1/\sqrt{x^2 + y^2 + z^2}$.  The
level surfaces of this function are spheres centered at the
origin.  On the other hand,
$$
  \nabla f = \lb \frac{-x}{(x^2 + y^2 + z^2)^{3/2}},
 \frac{-y}{(x^2 + y^2 + z^2)^{3/2}},
   \frac{-z}{(x^2 + y^2 + z^2)^{3/2}} \rb = - \frac 1{|\r|^3}\r.
$$
\outind{inverse square law}%
 This vector points toward the origin, so
it is perpendicular to a sphere centered at the origin.
\medskip
\centerline{\epsfbox{s3-30.ps}}
\medskip
 Notice that  in this example $|\nabla f| = |\r|/|\r|^3
 = 1/|\r|^2$, so it satisfies
the {\it inverse square law}.    Except for a constant,
the gravitational force due to a point mass at the
origin is given by this rule.  The function $f$ in this case
is the gravitational {\it potential energy function}. 
It is true for many
forces
(e.g., gravitational or electrostatic) that the gradient of
the potential energy function is the force.
\endexample

If we are working in $\R^2$ rather than $\R^3$,
then, $f(\r) = f(x,y) = c$ defines a family of {\it level curves\/}
rather than level surface.  As above,
the gradient $\nabla f$ is generally perpendicular to these
level curves. 

\subhead Other Forms of the Chain Rule \endsubhead
The most general chain rule tells us how to find
derivatives of  composites of functions $\R^n \to \R^m$ for
appropriate combinations of $m$ and $n$.   
We 
consider one special cases here.   You will see how to generalize
easily to other cases.  Let  $w = f(x, y)$
describe a scalar valued function of two variables.  ($f:\R^2 \to \R$.)   
Suppose, in addition, $x = x(s,t)$ and $y = y(s,t)$ describe
two functions of two variables $s, t$.  (As we shall see later,
this amounts to a single function $\R^2 \to \R2$.)  Then we
may consider the composite function described by
$$
    w = h(s,t) = f(x(s,t), y(s,t)).
$$
The chain rule generates formulas for $\partial h/\partial s$
and $\partial h/\partial t$ as follows.  Partial derivatives are
ordinary derivatives computed with the assumption that all the
variables but one vary.   Hence,
 to
compute $\partial h/\partial t$, all we need
to do is to use (\ChRTwo), replacing $dh/dt$ by $\partial h/\partial t$,  
$dx/dt$ by $\partial x/\partial t$, and $dy/dt$ by $\partial y/\partial t$.
Similarly, for $\partial h/\partial s$.  We get
\nexteqn
$$\align
   \frac{\partial h}{\partial s} &= 
 \frac{\partial f}{\partial x}\frac{\partial x}{\partial s}
       +
    \frac{\partial f}{\partial y}\frac{\partial y}{\partial s}\\
   \frac{\partial h}{\partial t} &= 
 \frac{\partial f}{\partial x}\frac{\partial x}{\partial t}
       +
    \frac{\partial f}{\partial y}\frac{\partial y}{\partial t}.\tag\eqn
   \endalign
$$
Note in these formulas that the differentiation variable on the left
($s$ or $t$) must agree with the ultimate differentiation variable on
the right.

\nextex
\example{Example \en}
Let  $f(x,y) = \sqrt{x^2 + y^2}, x = r\cos \theeta, y = r\sin \theeta$.
Here, the intermediate variables are $x, y$ and the ultimate
independent variables are $r, \theeta$.
We have
$$
\align
   \frac{\partial f}{\partial x}
 &= \frac x{\sqrt{x^2 + y^2}} = \frac{r\cos\theeta}r = \cos\theeta \\
   \frac{\partial f}{\partial y}
 &= \frac y{\sqrt{x^2 + y^2}} = \frac{r\sin\theeta}r = \sin\theeta .
\endalign $$
Also,
$$\alignat 2
   \frac{\partial x}{\partial r} &= \cos\theeta &\qquad
   \frac{\partial y}{\partial r} &= \sin\theeta \\
   \frac{\partial x}{\partial \theeta} &= -r \sin\theeta &\qquad
   \frac{\partial y}{\partial \theeta} &= r\cos\theeta.
\endalignat
$$
Hence,
$$\align
 \frac{\partial h}{\partial r}
&= (\cos\theeta)(\cos\theeta) + (\sin\theeta)(\sin\theeta) = 1 \\
 \frac{\partial h}{\partial \theeta}
&= (\cos\theeta)(-r\sin\theeta) + (\sin\theeta)(r\cos\theeta) = 0.
\endalign $$
Note that one must substitute for $x, y$ in terms of $r, \theeta$
in the expressions for $\partial f/\partial x$ and
$\partial f/\partial y$ or one won't get a complete answer.

The simplicity of the answers is more easily seen if we do the
substitution before differentiating.
$$
  h(r,\theeta) = \sqrt{r^2\cos^2\theeta + r^2\sin^2\theeta} = r.
$$
Hence, $\partial h/\partial r = 1$ and $\partial h/\partial \theeta = 0$.
Again, this illustrates the point that the multidimensional chain rule
is not primarily a computation device.  However, we shall see
that its utility in
theoretical discussions in mathematics {\it and in applications\/}
more than justifies its use.
\endexample

\subhead A Confusing Point \endsubhead
%% To instructor:  Perhaps omit this in class, but assign problems.
The most difficult use of the chain rule is in situations like the
following.  Suppose $w = f(x,y,z)$,  $z = z(x,y)$, and we want the
partial derivatives of $w = h(x,y) = f(x,y,z(x,y))$ with respect
to $x$ and $y$.  The correct formulas are
$$
\align
\frac{\d h}{\d x} &= \frac{\d f}{\d x} + \frac{\d f}{\d z}\frac{\d z}{\d x} \\
\frac{\d h}{\d y} &= \frac{\d f}{\d y} + \frac{\d f}{\d z}\frac{\d z}{\d y}.
\endalign
$$
One way to see the truth of these formulas is as follows.   Suppose
we introduce new variables $s$ and $t$  with  $x = x(s,t) = s$,
$y = y(s,t) = t$, and $z = z(s,t)$.   Let $w = h(s,t) =
f(x(s,t), y(s,t), z(s,t)) = f(s, t, z(s,t))$. Then, according to the
chain rule
$$
\align
\frac{\d h}{\d s} 
&= \frac{\d f}{\d x}\frac{\d x}{\d s} + \frac{\d f}{\d z}\frac{\d z}{\d s}\\
    &=\frac{\d f}{\d x} (1) + \frac{\d f}{\d z}\frac{\d z}{\d s}.
\endalign
$$
A similar argument works for $\dfrac{\d h}{\d t}$.
We may now obtain the previous formulas by identifying $x$ with $s$ and
$y$ with $t$.

The source of the problem is confusing the {\it variables\/}
with functions.   Thus, writing $z = z(x,y)$ lacks precision since 
the name `$z$' should not be used both for the variable and the function
expressing it in terms of other variables.   However, this 
is a common `abuse of notation' in applications, since it allows us
to concentrate on the physical interpretation of the variables which
might otherwise be obscured by a more precise mathematical formalism.

	To see how confusing all this can be, let's consider a
thermodynamic application.  The entropy $s$ is a function
$s = s(p, v, T)$ of pressure $p$, volume $v$, and temperature
$T$.   Also, the pressure may be assumed to be a function 
$p = p(v,T)$.   Thus, ultimately, we may express the entropy
$s = s(v,T)$.  Note the several abuses of notation.  $s(p,v,T)$
and $s(v,T)$ refer of course to {\it different\/} functions.
With this notation, the above formulas give 
\outind{thermodynamics}%
$$
\align
\frac{\d s}{\d v} &= \frac{\d s}{\d v} + \frac{\d s}{\d p}\frac{\d p}{\d v} \\
\frac{\d s}{\d T} &= \frac{\d s}{\d T} + \frac{\d s}{\d p}\frac{\d p}{\d T} 
\endalign
$$
which suggests that we may cancel the common terms on both sides to
conclude that the remaining term is zero.  This is not correct, since,
as just mentioned, the two functions `$s$' begin differentiated are
different.   To clarify this, one should write the formulas
$$ 
\align
\left(\frac{\d s}{\d v}\right)_T &= \left(\frac{\d s}{\d v}\right)_{p,T} + 
\left(\frac{\d s}{\d p}\right)_{v,T}\left(\frac{\d p}{\d v}\right)_T \\
\left(\frac{\d s}{\d T}\right)_v &= \left(\frac{\d s}{\d T}\right)_{v,T} 
+ \left(\frac{\d s}{\d p}\right)_{v,T}\left(\frac{\d p}{\d T}\right)_v. 
\endalign
$$
Here, the additional subscripts tell us which variables are kept constant,
so by implication we may see which variables the given quantity
is supposed to be a function of.
\bigskip
\subhead Gradient in Polar Coordinates \endsubhead
As you have seen, it is sometimes useful to resolve vectors in
the plane
in terms of the polar unit vectors $\u_r$ and $\u_\theeta$.
We want to do this for the gradient of a function $f$
given initially by  $w = f(\r) = f(x,y)$.   Suppose
\outind{gradient in polar coordinates}%
\outind{polar coordinates, gradient in}%
\nexteqn
\xdef\EOne{\eqn}
$$
  \nabla f = A_r\u_r + A_\theeta\u_\theeta.
\tag\eqn
$$
For a particle moving on a curve in the plane, the chain rule
tells us
\nexteqn
\xdef\ETwo{\eqn}
$$
  \frac{dw}{dt} = \nabla f\cdot \frac{d\r}{dt}.\tag\eqn
$$
On the other hand,
\nexteqn
\xdef\EThree{\eqn}
$$
    \frac{d\r}{dt} = \frac{dr}{dt}\u_r + r\frac{d\theeta}{dt}\u_\theeta.
\tag\eqn
$$
Putting (\EOne) and (\EThree) in (\ETwo) yields
\nexteqn
\xdef\EF{\eqn}
$$
\frac{dw}{dt} = 
   (A_r\u_r + A_\theeta\u_\theeta)\cdot(
    \frac{dr}{dt}\u_r + r\frac{d\theeta}{dt}\u_\theeta)
= A_r\frac{dr}{dt} + A_\theeta r\frac{d\theeta}{dt}.
\tag\eqn
$$
On the other hand,  we can write $w = g(r,\theeta) = f(x,y)
 = f(r\cos\theeta, r\sin\theeta)$, so applying the chain rule
directly to $g$, gives
\nexteqn
$$
 \frac{dw}{dt} = \frac{\partial g}{\partial r}\frac{dr}{dt}
      + \frac{\partial g}{\partial\theeta}\frac{d\theeta}{dt}.
\tag\eqn
$$
We now argue that since the curve could be anything, so could
$dr/dt$ and $d\theeta/dt$.  Hence, the coefficients of these
two quantities in (\EF) and (\eqn) must be the same.
Hence,  $A_r = \partial g/\partial r$ and $A_\theeta r =
\partial g/\partial\theeta$, i.e.,
$A_\theeta  =
(1/r)\partial g/\partial\theeta$.  It follows that
$$
   \nabla f = \frac{\partial g}{\partial r}\u_r +
     \frac 1r \frac{\partial g}{\partial \theeta}\u_\theeta
$$
where $g$ is the function expressing the desired quantity in
polar coordinates.

\mar{s3-31.ps}
\example{Example \GravEx, {\rm revisited}}  Let
$f(x,y) = 1/\sqrt{x^2 + y^2} = 1/r = g(r,\theeta)$.  Notice that
this is what we would get if we restricted Example \GravEx\ to
the $x,y$-plane by setting $z = 0$.   Then
$\partial g/\partial r = -(1/r^2)$ and
$\partial g/\partial \theeta = 0$.  Hence,
$$
 \nabla f = -\frac 1{r^2}\u_r.
 $$
This is  the same answer we got previously except that we
are restricting attention to the $x,y$-plane.

A similar analysis can be done in space if one uses the
appropriate generalization of polar coordinates, in this
case what are called {\it spherical coordinates}.   We shall
return to this later in the course.
\endexample
\bigskip
%Section 7 Exercises
\input chap3.ex7
\bigskip
\nextsec{Tangents to Level Sets and Implicit Differentiation}
%Section 8
\head Section \sn.  Tangents to Level Sets and Implicit
Differentiation \endhead

We saw in the previous section that
for a function $f:\R^2 \to \R$ of two variables,
 the gradient $\nabla f(\r_0)$
 is perpendicular
to the level curve
$f(\r) = c$
through $\r_0$,  
and similarly for functions $f:\R^3\to \R$ except that
the locus is a level {\it surface\/} instead.
\outind{tangent plane to level surface}%

\nextex
\example{Example \en}  Consider the locus in $\R^2$ of
the equation
$$
    x^3 + 3xy^2 + y = 15
$$
at the point  $(1,2)$.   The normal vector is $\nabla f (1,2)$
for $f(x,y) = x^3 + 3xy^2 + y$.  That is, the normal vector
is
$$ 
   \lb 3x^2 + 3y^2, 6xy + 1 \rb = \lb 15, 13 \rb.
$$
The tangent line will be characterized by the relation
$$
    \nabla f(\r_0)\cdot(\r - \r_0) = 0
$$
which in this case becomes
$$
     \lb 15, 13 \rb \cdot \lb x - 1, y - 2 \rb =
      15(x - 1) + 13(y - 2) = 0.
$$
Simplifying, this gives
$$
    15x + 13y = 41.
$$
\endexample

\mar{s3-32.ps}
\nextex
\example{Example \en}
Consider the hyperboloid of one sheet with equation
$$
  x^2 + 2y^2 - z^2 = 2.
$$
at $(1, 1, 1)$.  The normal vector is $\nabla f(1, 1, 1)$
where $f(x,y,z) = x^2 + 2y^2 - z^2$.  Thus, it is
$$
\lb 2x, 4y, -2z \rb = \lb 2, 4, -2 \rb.
$$
The tangent plane will be characterized by the equation
$\nabla f(\r_0)\cdot(\r - \r_0) = 0$ which in this case
becomes
$$
   2(x - 1) + 4(y - 1) - 2(z - 1) = 0
$$
or
$$
  2x + 4y - 2z = 2.
$$

\mar{s3-33.ps}
\endexample

There is a subtle point involved here.  What justification do we
have for believing that an equation of the form
$f(x,y) = c$ defines what we can honestly call a curve
in $\R^2$, and what justification is there for calling the
line above a tangent?  Similar questions can be
asked in $\R^3$ about the
 locus of $f(x,y,z) = c$ and the corresponding plane.
In fact, there are simple examples of where this would not
be a reasonable use of language.  For example, the locus in $\R^2$
of $x^2 + y^2 = 0$ is the {\it point\/} $(0,0)$, and it
certainly is not a curve in any ordinary sense.  Similarly,
 we saw previously that the locus in
$\R^3$ of $x^2 + y^2 - z^2 = 0$ is a (double)
cone, which looks like a surface all right, but it does not have
a well defined tangent plane at the origin.
If you look carefully at both these examples, you will see what
went wrong; in each case the gradient $\nabla f$ vanishes at
the point under consideration.  It turns out that if $\r_0$
is a point satisfying the equation $f(\r) = c$ and $\nabla f(\r_0)
\not= 0$, then the level set looks like what it should look like
and the locus of $\nabla f(\r_0)\cdot (\r - \r_0) = 0$
makes sense as a tangent (line or plane) to that level set.
\medskip
\centerline{\epsfbox{s3-34.ps}}
\medskip
The basis for all of this is a deep theorem called the
{\it implicit function theorem}.  We won't try to prove this
\outind{implicit function theorem}%
theorem here, or even to state it very precisely, but we
indicate roughly what it has to do with the above discussion.
Consider first the case of a level curve $f(x,y) = c$ in
$\R^2$.   Let $(x_0, y_0)$ be a point on that curve, where
$\nabla f(x_0, y_0) \ne \bold 0$.  The implicit function theorem
says that (subject to reasonable smoothness assumptions on $f$)
the curve is identical with the graph of some function $g$
of one variable, at least if we stay close enough to the point
$(x_0,y_0)$.  Moreover, the tangent line to the graph is the
same as the line obtained from the equation $\nabla f(\r_0)
\cdot (\r -\r_0) = 0$.   
\medskip
\centerline{\epsfbox{s3-35.ps}}
\medskip

\nextex
\example{Example \en}
The locus of $f(x,y) = x^2 + y^2 = 1$ is a circle of radius $1$
centered at the origin.  The gradient $\nabla f = \lb 2x, 2y \rb$
does not vanish at any point on the circle. If we fix a point $(x_0,y_0)$
with $y_0 > 0$ (top semicircle),
then in the vicinity of that point, the circle can be identified
with the graph of the function $y = g(x) = \sqrt{1 - x^2}$.
If $y_0 < 0$, we have to use $y = g(x) = -\sqrt{1 - x^2}$ instead.
It is not hard to check that the tangent line is the same whichever
method we use to find it.  

There is one problem.  Namely, at the points $(1,0)$ and $(-1,0)$
the tangent lines are vertical, so neither can be obtained as
a tangent to the graph of a function given by $y = g(x)$.  (The
slopes would be infinite.)   Instead, we have to reverse the
roles of $x$ and $y$ and use the graph of $x = g(y) = \sqrt{1 - y^2}$
near the point $(1,0)$ or $x = g(y) = -\sqrt{1 - y^2}$ near the
point $(-1,0)$.   
\endexample

\mar{s3-36.ps}
Similar remarks apply to level surfaces $f(x,y,z) = c$ in
$\R^3$ except that the conclusion is that in the neighborhood
of a point at which $\nabla f$ does not vanish the level set
can be identified with the graph of a function $g$ of two
variables.  In particular, it really looks like a surface, and
the equations of the tangent plane work out right.  (If the
normal vector $\nabla f$  points in a horizontal direction,
i.e., $f_z = 0$, you may have to try  $x = g(y,z)$ or $y = g(x,z)$
rather than $z = g(x,y)$.)

 
Having noted that level sets can be thought of (at least locally)
as graphs of functions, we should note that the reverse is
also true.  For example, suppose $f:\R^2 \to \R$ is a function
of two variables.  Its graph is the locus of $z = f(x,y)$. If
we define $F(x,y,z) = z- f(x,y)$, the graph is the zero level
set
$$
   F(x,y,z) = z - f(x,y)= 0.
$$
This remark allows us to treat the theory of tangent planes to
graphs as a special case
of the theory of tangent planes to level sets. 
The normal
vector is $\nabla F = \lb -f_x, -f_y, 1 \rb$ and the equation
of the tangent plane is $\lb -f_x(\r_0), -f_y(\r_0), 1 \rb
\cdot \lb x - x_0, y - y_0, z - z_0 \rb = 0$.

\nextex
\example{Example \en}
Consider the graph of the function $z = xy$ to be the
level set of the function
$$  F(x,y,z) = z - xy = 0.
$$
At $(0,0,0)$,
$\nabla F = \lb -y, -x, 1 \rb = \lb 0, 0, 1 \rb$.  Hence, the
equation of the tangent plane
is
$$
    0(x - 0) + 0(y - 0) -1(z - 0)  = 0
$$
or
$$ z = 0.$$
Thus, the tangent plane to the graph (which is a hyperbolic paraboloid)
at $(0,0,0)$ is the $x,y$-plane.  In particular, note that it
actually intersects the surface in {\it two lines}, the
$x$ and $y$ axes.  
\endexample 

\subhead Implicit Differentiation \endsubhead
The above discussion casts some light on the problem of
``implicit differentiation''.  You recall that this is
a method for finding $dy/dx$ in a situation in which
$y$ may not be given explicitly in terms of $x$, but
rather it is assumed there is a functional
relation $y = y(x)$ which is consistent with a relation
$$
    f(x,y) = c
$$
between $y$ and $x$.
\outind{implicit differentiation}%

\nextex
\example{Example \en}   Suppose $x^2 + y^2 = 1$ and find
$dy/dx$.  To solve this we differentiate the relation using
the usual rules to obtain
$$
    2x + 2y\frac{dy}{dx} = 0
$$
which can be solved 
$$
    \frac{dy}{dx} = -\frac xy.
$$
Note that the answer depends on both $x$ and $y$, so for example
it would be different for $y > 0$ (the top semi-circle) and
$y < 0$ (the bottom semi-circle).  This is consistent with the
fact that in order to pick out a {\it unique\/} functional
relationship $y = y(x)$, we must specify where on the circle
we are.  In addition, the method does not make sense if $y = 0$,
i.e., at the points $(\pm 1, 0)$.  As we saw above, it would
be more appropriate to express $x = x(y)$ as a function of $y$
in the vicinity of those points.
\endexample

The process of implicit differentiation can be explained in
terms of the tangent line to a level curve as follows.  First,
rewrite the equation of the tangent to $f(x,y) = c$ in
differential form
\nexteqn
$$
   \nabla f\cdot d\r = \frac{\partial f}{\partial x} dx
      +
   \frac{\partial f}{\partial y} dy = 0.\tag\eqn
$$
\medskip
\centerline{\epsfbox{s3-37.ps}}
\medskip
This is just a change of notation where we drop the subscript
$0$ in describing the point on the curve and we use
$d\r$ rather than $\r - \r_0$ for the displacement along
the tangent line.  (We usually think of $d\r$ as being very small
so we don't have to distinguish between the displacement in
the tangent direction and the displacement along the curve,
at least if we are willing to tolerate a very small error.)
(\eqn) can be solved for $dy$ by writing
$$
    dy  =  - \frac{\partial f/\partial x}  {\partial f/\partial y} dx
$$
provided the denominator $f_y \ne 0$.  On the other hand, the
implicit function theorem tells us that near the point of tangency
we may assume that the graph is the graph of a function
given by $y = g(x)$, and the tangent line to that graph would
be expressed in differential notation
$$
    dy = g'(x) dx = \frac{dy}{dx} dx.
$$
Since it is the same tangent line in either case, we conclude
\nexteqn
\xdef\EqImpTwo{\eqn}
$$
    \frac{dy}{dx} = 
  - \frac{\partial f/\partial x} 
   {\partial f/\partial y}.
$$
(Note that the assumption $f_y \ne 0$ at the point of tangency
plays an additional role here.  That condition insures that the
normal vector $\nabla f$ won't point horizontally, i.e., that
the tangent line is not vertical.  Hence, it makes sense to
consider the tangent as a tangent line to the graph of a function
$y = g(x)$.) 

\example{Example \en, {\rm revisited}}   For $$f(x,y) = x^2 + y^2 =
1,$$ we have $f_x(x,y) = 2x, f_y(x,y) = 2y$, so (\EqImpTwo)
tells us $dy/dx = - x/y$, just as before.
 \endexample

Similar reasoning applies to level surfaces in $\R^3$.   Let
$$
    f(x,y,z) = c
$$
define such a level surface, and write the equation of the
tangent plane in differential notation
\nexteqn
$$
  \nabla f\cdot d\r = 
    \frac{\partial f}{\partial x} dx
      +
   \frac{\partial f}{\partial y} dy 
+   \frac{\partial f}{\partial z} dz = 0.\tag\eqn
$$
Solving for $dz$ yields
$$
    dz = 
  - \frac{\partial f/\partial x} 
   {\partial f/\partial z} dx
  - \frac{\partial f/\partial y} 
   {\partial f/\partial z} dy
  $$
provided the denominator $f_z \ne 0$.   On the other hand, at
such a point, the implicit function theorem allows us to
identify the surface near the point with the graph of a function 
 $z = g(x,y)$. 
 Moreover,  we may
write the equation of the tangent plane to the graph in
differential form as
$$
    dz = \frac{\partial g}{\partial x} dx + 
         \frac{\partial g}{\partial y} dy.
$$
Since it is the same tangent plane in either case, we conclude
that
$$
\align
    \frac{\partial g}{\partial x} &=  
  - \frac{\partial f/\partial x} 
   {\partial f/\partial z}  \\
         \frac{\partial g}{\partial y} 
  & = - \frac{\partial f/\partial y} 
   {\partial f/\partial z}.
\endalign
  $$

\nextex
\example{Example \en} 
Suppose $xyz + xz^2 + z^3 =  1$.  Assuming $z = g(x,y)$, find
$\partial z/\partial x$ and $\partial z/\partial y$ in general
and also for $x = 1, y = -1, z = 1$.   To solve this,
we put $f(x,y,z) = xyz + xz^2 + z^3$ and set its total differential
to zero
$$
  df = f_x dx + f_y dy + f_z dz = (yz + z^2)dx + (xz)dy + (xy + 2xz
 + 3z^2) dz = 0.
$$
This can be rewritten
$$
  dz = -\frac{yz + z^2}{xy + 2xz + 3z^2}dx - \frac{xz}
{xy + 2xz + 3z^2} dy
$$
so
$$
\align
\frac{\partial z}{\partial x} &=    
   -\frac{yz + z^2}{xy + 2xz + 3z^2} \\
\frac{\partial z}{\partial y} &=    
 - \frac{xz}{xy + 2xz + 3z^2}
\endalign
$$
provided the denominator does not vanish.  At $(1, -1, 1)$ we
have $\partial z/\partial x = 0/4 = 0$ and $\partial z/
\partial y = 1/4$.

There is another way this problem could be done which parallels
the approach you learned in your previous course.   For example,
to find $\partial z/\partial x$, just apply the ``operator''
$\partial/\partial x$ to the equation
$xyz + xz^2 + z^3 = 1$  under the assumption that $z = g(x,y)$
but $x,y$ are independent.  We get
$$
  yz + xy\frac{\partial z}{\partial x} 
     + z^2 + 2xz\frac{\partial z}{\partial x} +
     3z^2\frac{\partial z}{\partial x} = 0.
$$
This equation can be solved for $\partial z/\partial x$ and
we get the same answer as above.  (Check it!)
\endexample

\subhead A Theoretical Point \endsubhead
Consider the level surface $f(\r) = c$ at a point
where the gradient $\nabla f$ does not vanish.  In showing
that the gradient is perpendicular to the level surface
(in the previous section), we claimed that every possible
tangent vector to the surface at the point of
tangency is in fact
tangent to some curve lying in the surface.  A moment's
thought shows there are some problems with this
assertion.  First, how
do we know that the level set looks like a surface,
(as opposed say to a point)
and if it does, what do we mean by its tangent
plane?  These issues
are settled by using the implicit function theorem.
For as long as $\nabla f$ does not vanish at the point,
that theorem allows us to view the surface in the vicinity
of the given point as the graph of some function.
However, the graph of a function certainly has the right
properties for our concept of ``surface''.   Moreover,
there is no problem
with tangent vectors to a graph, and any such vector determines
a {\it plane section perpendicular to the domain of the function\/}
which intersects the graph in a curve with the desired tangent.
(See the diagram.)
\smallskip
\mar{s3-38.ps}
\smallskip
%Section 8 Exercises
\input chap3.ex8
\bigskip
\nextsec{Higher Partial Derivatives}
\head Section \sn.  Higher Partial Derivatives. \endhead 
Just as in the single variable case, you can continue taking
derivatives in the multidimensional case.  However, because 
there is more than one independent variable, the situation is a
bit more complicated.
\outind{partial derivative, higher}%
\outind{partial derivative, mixed}%

\nextex
\example{Example \en}
Let $f(x,y) = x^2y + y^2$.  Then
$$
  \frac{\partial f}{\partial x} = 2xy\qquad
  \frac{\partial f}{\partial y} = x^2 + 2y.
$$
Each of these can be differentiated with respect to either
$x$ or $y$ and the results are denoted
$$\alignat 2
  \frac{\partial^2 f}{\partial x^2} &=
  \frac{\partial }{\partial x}\left( \frac{\partial f}{\partial x}\right)
 = 2y
  &\qquad 
  \frac{\partial^2 f}{\partial x\partial y} &=
  \frac{\partial }{\partial x}\left( \frac{\partial f}{\partial y}\right)
 = 2x \\
  \frac{\partial^2 f}{\partial y\partial x} &=
  \frac{\partial }{\partial y}\left( \frac{\partial f}{\partial x}\right) = 2x 
&\qquad
  \frac{\partial^2 f}{\partial y^2} &=
  \frac{\partial }{\partial y} \left(\frac{\partial f}{\partial y}\right) = 2.
\endalignat
$$
\endexample

All these are called {\it second order partial derivatives}.
Note that the order in which the operations are performed is from
right to left; the operation closer to the function is performed
first.  $\dfrac{\partial^2 f}{\partial x\partial y}$ and 
$\dfrac{\partial^2 f}{\partial y\partial x}$ are called {\it mixed partials}.
Other notation for partial derivatives  is
$f_{xx}, f_{xy},  
f_{yx}$, and $f_{yy}$.  However, just to make life difficult for you,
the order is different.  For example, $f_{xy}$ means first differentiate
with respect to $x$ and then differentiate the result with respect
to $y$.

   Fortunately,
it doesn't usually make any difference which order you do the
operations.  For example, in  Example \en, we have
$$ 
\align
  \frac{\partial^2 f}{\partial x\partial y} &=
  \frac{\partial }{\partial x} \frac{\partial f}{\partial y} = 2x \\
  \frac{\partial^2 f}{\partial y\partial x} &=
  \frac{\partial }{\partial y} \frac{\partial f}{\partial x} = 2x 
\endalign
$$
so the mixed partials are equal.   The following theorem gives us conditions
under which we can be sure such is the case.
We state if for functions of two variables, but its analogue
holds for functions of any number of variables.

\nextthm
\proclaim{Theorem \cn.\tn}  Let $z = f(x,y)$ denote a function
of two variables defined on some open set in
$\R^2$.   Assume the partial derivatives
 $f_x, f_y$, and $f_{xy}$ are
defined and $f_{xy}$ 
is continuous on that set.  Then $f_{yx}$  exists and
$f_{xy}(x,y) = f_{yx}(x,y)$.
\endproclaim

A function with continuous second order partial derivatives is
usually called $\Cal C^2$.   This is more stringent than the
\outind{$\noexpand\Cal C^2$-function}%
condition of being $\Cal C^1$ (having continuous first order
partials).  It will almost always be true that functions you
have to deal with in applications are $\Cal C^2$ except possibly
for an isolated set of points.

Clearly, we can continue this game ad infinitum.  There are
8 possible 3rd order derivatives for a function of two variables:
$$
  f_{xxx}, f_{yxx}, f_{xyx}, f_{xxy}, f_{xyy}, f_{yxy},
f_{yyx}, f_{yyy}.
$$
(These could also be denoted $\partial^3 f/\partial x^3,
\partial^3 f/\partial x^2\partial y$, etc.)  However, for
sufficiently smooth functions, the 2nd, 3rd, and 4th are the
same, as are the 5th, 6th, and 7th.

How many second order partial derivatives are there for a
function of 3 variables $x, y$, and $z$?   How many are the
same for $\Cal C^2$ functions?

\subhead Proof of the Theorem \endsubhead
We include here a proof of Theorem \cn.\tn\ because some of you
might be curious to see how it is done.  However, you will be
excused if you choose to skip the proof.

The proof is based on the Mean Value Theorem (as was the
proof of Theorem 3.1).  
%Ref
We have
$$
  f_{yx}(x,y) = \lim_{\Delta x \to 0}
\frac{f_y(x+ \Delta x,y) - f_y(x, y)}{\Delta x}.
$$
However,
$$
\align
f_y(x,y) &=
  \lim_{\Delta y \to 0}
\frac{f(x,y + \Delta y) - f(x, y)}{\Delta y} \\
f_y(x + \Delta x,y) &=
  \lim_{\Delta y \to 0}
\frac{f(x + \Delta x,y +\Delta y) - f(x + \Delta x, y)}{\Delta y}.
\endalign
$$
Hence,
\nexteqn
$$
\align
f_{yx} &=
\lim_{\Delta y \to 0} \lim_{\Delta x \to 0}\left[
\frac{f(x + \Delta x,y +\Delta y) - f(x + \Delta x, y)}{\Delta x \Delta y}
-\frac{f(x,y + \Delta y) - f(x, y)}{\Delta x\Delta y}\right] \\
&= \lim_{\Delta y \to 0} \lim_{\Delta x \to 0}
\frac{f(x + \Delta x,y +\Delta y) - f(x + \Delta x, y)
 - f(x,y + \Delta y) + f(x, y)}{\Delta x\Delta y}.\tag\eqn
\endalign
$$
Call the expression in the numerator $\Delta$.
Now, put
$$
g(x) = 
f(x,y + \Delta y) 
 - f(x,y),
$$
so the dependence on $y$ is suppressed.  Note that
$$
g(x  + \Delta x) - g(x) =
f(x + \Delta x,y +\Delta y)
 - f(x+ \Delta x,y)
 - f(x, y + \Delta y)
 + f(x, y) = \Delta.
$$
 By the Mean Value Theorem,
$$
\Delta = g(x + \Delta x) - g(x) = g'(x_1)\Delta x
$$
for some $x_1$ between $x$ and $x + \Delta x$.  Remembering
what $g$ is, we get
$$
\Delta = (f_x(x_1, y + \Delta y) - f_x(x_1, y))\Delta x.
$$
(The differentiation in $g'$ was with respect to $x$.)
Now apply the Mean Value Theorem again to get
$$
\Delta = (f_{xy}(x_1,y_1)\Delta y)\Delta x
$$
for some $y_1$ between $y$ and $y + \Delta y$.  Note that
$(x_1, y_1) \to (x,y)$ as $\Delta x, \Delta y \to 0$.
Referring again to (\eqn), we have
$$
  f_{yx}(x,y) = 
\lim_{\Delta y \to 0} \lim_{\Delta x \to 0}\frac{\Delta}{\Delta x\Delta y}
    =
\lim_{\Delta y \to 0} \lim_{\Delta x \to 0}
f_{xy}(x_1,y_1) = f_{xy}(x,y).
$$
The last equality follows from the hypothesis that $f_{xy}$ is
continuous.

\subhead An Example \endsubhead
It is easy to given an example of a function for which the
mixed partials are not equal. Let
$$
 f(x,y) = xy\frac{x^2 - y^2}{x^2 + y^2}.
$$
This formula does not make sense for $(x,y) = (0,0)$
but it is not hard to see that $\lim_{(x,y)\to(0,0)}
f(x,y) = 0$.   Hence, we can extend the definition of the
function by defining $f(0,0) = 0$, and the resulting
function is continuous.

The mixed partial derivatives of this function at 
$(0,0)$ are not equal.  To see this, note first that
$$
f_x(x,y) = \frac{y(x^4 + 4x^2y^2 - y^4)}{(x^2 + y^2)^2}
$$
as long as $(x,y) \ne (0,0)$.  (That is a messy but routine
differentiation which you can work out for yourself.)  To
determine $f_x(0,0)$, note that for $y = 0$, we
get $f(x, 0) = 0$.  Hence, it follows that $f_x(x,0) = 0$
for every $x$ including $x = 0$.   We can now calculate
$f_{xy}(0,0)$ as
$$
f_{xy}(0,0) = 
\lim_{\Delta y \to 0}
 \frac{f_x(0, 0 + \Delta y) - f_x(0,0)}
{\Delta y} = 
\lim_{\Delta y \to 0}
\frac 1{\Delta y} 
\frac{\Delta y( - (\Delta y)^4)}{(\Delta y^2)^2} = -1.
$$
Calculating $f_{yx}(0,0)$
(in the other order) proceeds along the same lines.  However,
since $f(y,x) = -f(x,y)$, you can see that reversing the roles
of $x$ and $y$ will yield $+1$ instead, and indeed $f_{yx}(0,0)
= +1$.   Hence, the two mixed partials are not equal.

\subhead An Application to Thermodynamics \endsubhead
You may have been studying thermodynamics in your chemistry
course.   In thermodynamics, one studies the relationships
among variables called {\it pressure, volume, and temperature}.
\outind{thermodynamics}%
These are usually denoted  $p, v$, and $T$.  They are assumed
to satisfy some equation of state
$$
   f(p, v, T) = 0.
$$
For example, $pv - kT = 0$ is the law which is supposed to
hold for an ideal gas, but there are other more complicated laws such
as the van der Waals equation.  In any case, that means that
we can pick two of the three variables as the independent
variables and the remaining variable depends on them.
However, there is no preferred choice for independent
variables and one switches from
one to another, depending on the circumstances.
 In addition, one introduces
other quantities such as the internal energy ($u$), the entropy
($s$), the enthalpy, etc. which are all functions of the other
variables.  (In certain circumstances, one may use these other
variables as independent variables.)  

It is possible to state two of the basic laws of thermodynamics
in terms of entropy ($s$) and internal energy ($u$) as follows.
$$
\align 
T\left(\frac{\partial s}{\partial T}\right)_v
 & = 
\left(\frac{\partial u}{\partial T}\right)_v\qquad\text{First Law} \\
T\left(\frac{\partial s}{\partial v}\right)_T
 & = 
\left(\frac{\partial u}{\partial v}\right)_T + p
\qquad\text{Second Law}.
\endalign
$$
From these relations, it is possible to derive many others.
For example, from the first law, we get by differentiating with
respect to $v$,
$$
   T\frac{\partial^2 s}{\partial v\partial T}
 =
   \frac{\partial^2 u}{\partial v\partial T}.
$$
From the second law, we get by differentiating with respect to
$T$
$$
  \left(\frac{\partial s}{\partial v}\right)_T
+ T\frac{\partial^2 s}{\partial T\partial v}
 =
   \frac{\partial^2 u}{\partial T\partial v} + \left(\frac
{\partial p}{\partial T}\right)_v.
$$
Subtracting the first from the second and using the equality
of the mixed partials yields
$$
  \left(\frac{\partial s}{\partial v}\right)_T
 =
 \left(\frac
{\partial p}{\partial T}\right)_v.
$$
This is one of four relations called {\it Maxwell's relations}.
\bigskip
%Section 9 Exercises
\input chap3.ex9
\endchapter
\closeseg{chap3}
\enddocument
